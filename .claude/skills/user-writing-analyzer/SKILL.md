---
name: user-writing-analyzer
description: åˆ†æç”¨æˆ·çš„ Amazon è¯„è®ºï¼Œé€šè¿‡è„šæœ¬ç”Ÿæˆåˆ†æ Promptï¼Œå¼•å¯¼ Agent æ‰‹åŠ¨é€ä¸ªæå–æ‹¼å†™é”™è¯¯å’Œè¯­æ³•é”™è¯¯ä¹ æƒ¯ï¼Œå¹¶ç›´æ¥å†™å…¥ error_analysis_USERID.json æ–‡ä»¶ã€‚âš ï¸ ä¸¥ç¦æ‰¹é‡è„šæœ¬ç”Ÿæˆï¼Œå¿…é¡»ä¸€ä¸ªä¸€ä¸ªæ‰‹åŠ¨ç”Ÿæˆã€‚
allowed-tools: run_command, view_file, ask_user_question
---

# User-Writing-Analyzer (Spelling & Grammar)

æ­¤æŠ€èƒ½ç”¨äºè®© Agent (Claude) æ¨¡æ‹Ÿç‰¹å®šç”¨æˆ·çš„æ‹¼å†™å’Œè¯­æ³•ä¹ æƒ¯ï¼Œé€šè¿‡åˆ†æç”¨æˆ·çš„è¯„è®ºå†å²ï¼Œ**é€ä¸ªæ‰‹åŠ¨**æå–å…¶é”™è¯¯æ¨¡å¼ï¼Œç›´æ¥è¿½åŠ åˆ° `error_analysis_USERID.json` æ–‡ä»¶çš„æ•°ç»„ä¸­ã€‚

---

## ğŸš€ å¿«é€Ÿå‚è€ƒï¼ˆAIå¿…è¯»ï¼‰

### âš¡ æ ¸å¿ƒè¦æ±‚ï¼ˆ3æ¡å¿…é¡»éµå®ˆï¼‰

1. **æ¯æ‰¹æœ€å¤š10æ¡** - ä¸æ˜¯101æ¡ä¸€æ¬¡æ€§å®Œæˆï¼
2. **ç›´æ¥å†™å…¥æœ€ç»ˆæ–‡ä»¶** - ä½¿ç”¨æ ‡å‡† `.json` æ•°ç»„æ ¼å¼ï¼Œæ¯æ¡è¯„è®ºåˆ†æå®Œç«‹å³æ›´æ–°æ–‡ä»¶ï¼Œè¦æ±‚å¸¦ç¼©è¿›ã€‚
3. **å…¨é¢åˆ†æ** - åŒæ—¶è¯†åˆ«æ‹¼å†™é”™è¯¯ï¼ˆ10ç§ï¼‰å’Œè¯­æ³•é”™è¯¯ï¼ˆ7ç§ï¼‰ï¼Œä¸è¦é—æ¼ä»»ä½•ä¸€ç§é”™è¯¯ç±»å‹ã€‚

### ğŸ¯ æ‰§è¡Œæµç¨‹

```
å¼€å§‹ â†’ åˆ†æ10æ¡ â†’ è‡ªåŠ¨è´¨é‡æ£€æµ‹
  â†“                     â†“
é€šè¿‡ â†’ ç»§ç»­ä¸‹10æ¡     å¤±è´¥ â†’ åˆ é™¤é‡æ¥
  â†“                     â†“
é‡å¤ç›´åˆ°å®Œæˆ         é‡æ–°åˆ†æ
  â†“
å…¨éƒ¨å®Œæˆ â†’ å®Œæˆ
```

---

## â›”ğŸš¨ğŸš¨ ç»å¯¹ç¦æ­¢æ¡æ¬¾ - è¿åå³ä»»åŠ¡å¤±è´¥ ğŸš¨ğŸš¨â›”

1. âŒ **ä¸¥ç¦ä½¿ç”¨æ‰¹é‡å¤„ç†è„šæœ¬è¿›è¡Œ LLM åˆ†æè°ƒç”¨**
2. âŒ **ä¸¥ç¦ä¸ºäº†å‡‘æ•°è€Œç¼–é€ é”™è¯¯**
3. âŒ **é—æ¼ä»»ä½•ä¸€ç§é”™è¯¯ç±»å‹** - å¿…é¡»åŒæ—¶æ£€æŸ¥æ‹¼å†™é”™è¯¯å’Œè¯­æ³•é”™è¯¯

---

## ğŸ“‚ æ‹¼å†™é”™è¯¯åˆ†ç±» (Spelling Errors - 10 Types)

### 1. Deletion (æ¼è¾“/ç¼ºå¤±å­—æ¯)
- `colr` -> `color` (ç¼ºå¤± `o`)

### 2. Insertion (å¤šè¾“/å¤šä½™å­—æ¯)
- `accross` -> `across` (å¤šäº† `c`)

### 3. Transposition (æ¢ä½/ç›¸é‚»å­—æ¯é¢ å€’)
- `teh` -> `the`, `thier` -> `their`

### 4. Scramble (å¤æ‚æ··ä¹±/å¤šé‡é”™è¯¯)
- `definitly` -> `definitely` (æ—¢æœ‰ç¼ºå¤±åˆæœ‰æ›¿æ¢)

### 5. Substitution (æ›¿æ¢/å­—æ¯æ›¿æ¢)
- `wprk` -> `work` (p æ›¿æ¢äº† o)

### 6. Homophone (åŒéŸ³è¯/éŸ³è¿‘å½¢å¼‚)
- `there` -> `their`, `your` -> `you're` (æ³¨æ„ï¼šåŸè¯å¿…é¡»æ˜¯åˆæ³•å•è¯ï¼Œä½†ç”¨æ³•é”™è¯¯)

### 7. Suffix (åç¼€é”™è¯¯/è¯å°¾å˜å½¢é”™è¯¯)
- `runing` -> `running`, `boxs` -> `boxes`

### 8. Hard Word (éš¾è¯/ç”Ÿåƒ»è¯é”™è¯¯)
- `fuchsia` -> `fushia`

### 9. Extra Space (å¤šä½™ç©ºæ ¼)
- `note book` -> `notebook`

### 10. Extra Hyphen (å¤šä½™è¿å­—ç¬¦)
- `note-book` -> `notebook`

---

## ğŸ“‚ è¯­æ³•é”™è¯¯åˆ†ç±» (Grammar Errors - 7 Types)

### 1. Agreement (ä¸€è‡´æ€§é”™è¯¯)
ä¸»è°“ä¸ä¸€è‡´ã€å•å¤æ•°ä¸ä¸€è‡´ã€å† è¯ä½¿ç”¨é”™è¯¯ç­‰
- `it is` -> `they are` (ä¸»è¯­æ˜¯å¤æ•°)
- `these kit is` -> `these kits are` (æ•°ä¸ä¸€è‡´)
- `a rayon` -> `rayon` (ä¸å¯æ•°åè¯ä¸è¯¥åŠ å† è¯)

### 2. Collocation (æ­é…é”™è¯¯)
è¯ç»„æ­é…ä¸è‡ªç„¶ï¼Œå›ºå®šç”¨æ³•é”™è¯¯
- `between 4 or 5` -> `between 4 and 5` (betweenåº”è¯¥æ­é…and)
- `first glance` -> `first attempt` (è¯­å¢ƒæ­é…ä¸å½“)
- `fit in the scheme` -> `fit into the scheme` (ä»‹è¯æ­é…é”™è¯¯)

### 3. Preposition (ä»‹è¯é”™è¯¯)
ä»‹è¯ç¼ºå¤±ã€é”™è¯¯æˆ–å¤šä½™
- `slots the types` -> `slots for the types` (ç¼ºå¤±ä»‹è¯for)
- `excel that set` -> `excel at that set` (ç¼ºå¤±ä»‹è¯at)
- `range of light to dark` -> `range from light to dark` (ä»‹è¯é”™è¯¯)

### 4. Pronoun (ä»£è¯é”™è¯¯)
ä»£è¯æŒ‡ä»£ä¸æ˜ã€ä½¿ç”¨é”™è¯¯
- `what I consider` -> `which I consider` (å…³ç³»ä»£è¯é”™è¯¯)
- `who` -> `that` (éäººç§°ä»£è¯åº”ç”¨that)
- `these` -> `they` (æŒ‡ä»£ä¸æ˜)

### 5. Suffix (åç¼€/è¯å½¢é”™è¯¯)
æ¯”è¾ƒçº§ã€è¯æ€§åç¼€ã€åŠ¨è¯å½¢æ€é”™è¯¯
- `more fine` -> `finer` (æ¯”è¾ƒçº§åº”ä¸ºfiner)
- `coarsely` -> `coarse` (åº”ä¸ºå½¢å®¹è¯coarse)
- `to using` -> `to use` (ä¸å®šå¼åº”ä¸ºuse)

### 6. Homophone (åŒéŸ³è¯-è¯­æ³•ç±»)
åŠ¨è¯æ—¶æ€æˆ–å½¢å¼ç›¸å…³çš„åŒéŸ³è¯è¯¯ç”¨
- `lay down` -> `lie down` (åŠ¨è¯æ—¶æ€é”™è¯¯ï¼Œlayæ˜¯è¿‡å»å¼ï¼Œlieæ‰æ˜¯åŸå½¢)

### 7. Hyphenation (è¿å­—ç¬¦é”™è¯¯)
å¤åˆå½¢å®¹è¯ç¼ºå°‘æˆ–å¤šä½™è¿å­—ç¬¦
- `good size` -> `good-sized` (å¤åˆå½¢å®¹è¯åº”åŠ è¿å­—ç¬¦)
- `cross stitch thread` -> `cross-stitch thread` (å¤åˆè¯åº”åŠ è¿å­—ç¬¦)

---

## æ‰§è¡Œæ­¥éª¤

### 1. ç”Ÿæˆ Prompt
```bash
python3 /home/wlia0047/ar57/wenyu/.claude/skills/user-writing-analyzer/generate_writing_prompts.py \
    --user_id AG7EF0SVBQOUX \
    --output /home/wlia0047/ar57/wenyu/result/writing_analysis/prompts.json
```

### 2. æ‰‹åŠ¨åˆ†æå¹¶å†™å…¥
åˆ†ææ¯ä¸ª Promptï¼Œæå–é”™è¯¯ï¼Œå¹¶ä½¿ç”¨ä»¥ä¸‹è¾“å‡ºæ¨¡æ¿ï¼ˆ**ä»…åŒ…å«å‘ç°é”™è¯¯çš„ç±»åˆ«ï¼Œè‹¥ç±»åˆ«ä¸ºç©ºåˆ™ä¸å†™å…¥**ï¼‰ï¼š

```json
{
  "review_index": 0,
  "spelling_errors": {
    "Deletion": [
      { "original": "colr", "corrected": "color", "fragment": "...pretty colr for...", "reason": "Missing 'o'" }
    ]
  },
  "grammar_errors": {
    "Agreement": [
      { "original": "it is", "corrected": "they are", "fragment": "...gemstones and it is...", "reason": "Subject 'gemstones' is plural" }
    ]
  }
}
```

**å†™å…¥é€»è¾‘ç¤ºä¾‹ï¼ˆæ‰‹åŠ¨ç»´æŠ¤ JSON æ•°ç»„ï¼Œæ³¨æ„è¿‡æ»¤ç©ºç±»åˆ«ï¼‰ï¼š**
```python
import json
import os

# é…ç½®
output_dir = "/home/wlia0047/ar57/wenyu/result/writing_analysis"
user_id = "USER_ID_HERE" # è®°å¾—ä¿®æ”¹ID
analysis_file = os.path.join(output_dir, f"error_analysis_{user_id}.json")
stats_file = os.path.join(output_dir, f"error_stats_{user_id}.json")

# æ„å»ºç»“æœæ—¶è¿‡æ»¤æ‰ç©ºåˆ—è¡¨ (raw_errors æ¥è‡ªä½ çš„æ‰‹åŠ¨åˆ†æ)
# raw_errors = { ... } 

# è¿‡æ»¤ç©ºç±»åˆ«
spelling_errors = {k: v for k, v in raw_errors["spelling_errors"].items() if v}
grammar_errors = {k: v for k, v in raw_errors["grammar_errors"].items() if v}

# 1. å†™å…¥ç»“æœæ–‡ä»¶ (ä»…å½“æœ‰é”™è¯¯æ—¶)
if spelling_errors or grammar_errors:
    res = {
        "review_index": 0, # è®°å¾—ä¿®æ”¹ç´¢å¼•
        "spelling_errors": spelling_errors,
        "grammar_errors": grammar_errors
    }

    if os.path.exists(analysis_file):
        with open(analysis_file, 'r') as f:
            data = json.load(f)
    else:
        data = []

    data.append(res)
    with open(analysis_file, 'w') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"âœ… å·²å†™å…¥é”™è¯¯åˆ†æ: {analysis_file}")
else:
    print("â„¹ï¸ æœªå‘ç°é”™è¯¯ï¼Œè·³è¿‡å†™å…¥ç»“æœæ–‡ä»¶ã€‚")

# 2. æ›´æ–°ç»Ÿè®¡æ–‡ä»¶ (æ— è®ºæ˜¯å¦æœ‰é”™ï¼Œéƒ½å¯ä»¥è®°å½•å·²åˆ†ææ•°é‡ï¼Œè¿™é‡Œä¸»è¦ç»Ÿè®¡é”™è¯¯æ•°)
if os.path.exists(stats_file):
    with open(stats_file, 'r') as f:
        stats = json.load(f)
else:
    stats = {"spelling": {}, "grammar": {}, "total_reviews_analyzed": 0}

stats["total_reviews_analyzed"] = stats.get("total_reviews_analyzed", 0) + 1

# ç»Ÿè®¡æ­¤æ¡è¯„è®ºçš„é”™è¯¯
for category, errors in spelling_errors.items():
    stats["spelling"][category] = stats["spelling"].get(category, 0) + len(errors)

for category, errors in grammar_errors.items():
    stats["grammar"][category] = stats["grammar"].get(category, 0) + len(errors)

# ğŸ”¥ é‡è¦ï¼šæ·»åŠ æ±‡æ€»å­—æ®µï¼ˆæ¯æ¬¡æ›´æ–°æ—¶é‡æ–°è®¡ç®—ï¼‰
spelling_total = sum(stats.get("spelling", {}).values())
grammar_total = sum(stats.get("grammar", {}).values())
stats["spelling_total"] = spelling_total
stats["grammar_total"] = grammar_total
stats["total_errors"] = spelling_total + grammar_total

# ğŸ†• è®¡ç®—æ€»å•è¯æ•°å’Œé”™è¯¯ç‡ï¼ˆéœ€è¦è¯»å–æ‰€æœ‰è¯„è®ºï¼‰
# æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ä½ æœ‰æ‰€æœ‰è¯„è®ºçš„æ–‡æœ¬ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æ›¿æ¢ä¸ºä½ çš„æ•°æ®æº
# stats["total_words"] = calculate_total_words(all_reviews)  # éœ€è¦å®ç°è¿™ä¸ªå‡½æ•°
# stats["errors_per_100_words"] = round((stats["total_errors"] / stats["total_words"]) * 100, 2)

with open(stats_file, 'w') as f:
    json.dump(stats, f, indent=2, ensure_ascii=False)
print(f"ğŸ“Š å·²æ›´æ–°ç»Ÿè®¡æ•°æ®: {stats_file}")
```

**è¾“å‡ºæ ¼å¼ï¼š**
```json
[
  {
    "review_index": 0,
    "spelling_errors": {
      "Deletion": [],
      "Insertion": []
    },
    "grammar_errors": {
      "Agreement": [],
      "Collocation": []
    }
  },
  ...
]
```

### ğŸ“Š ç»Ÿè®¡æ–‡ä»¶æ ¼å¼è¦æ±‚

ç»Ÿè®¡æ–‡ä»¶ `error_stats_{USER_ID}.json` **å¿…é¡»**åŒ…å«ä»¥ä¸‹å­—æ®µï¼š

```json
{
  "spelling": {
    "Deletion": 2,
    "Insertion": 1,
    ...
  },
  "grammar": {
    "Agreement": 5,
    "Collocation": 3,
    ...
  },
  "total_reviews_analyzed": 101,
  "spelling_total": 10,
  "grammar_total": 31,
  "total_errors": 41,
  "total_words": 7940,
  "errors_per_100_words": 0.52
}
```

**å­—æ®µè¯´æ˜ï¼š**
- `spelling`: æ‹¼å†™é”™è¯¯æŒ‰ç±»åˆ«ç»Ÿè®¡
- `grammar`: è¯­æ³•é”™è¯¯æŒ‰ç±»åˆ«ç»Ÿè®¡
- `total_reviews_analyzed`: å·²åˆ†æçš„æ€»è¯„è®ºæ•°
- `spelling_total`: æ‹¼å†™é”™è¯¯æ€»æ•°ï¼ˆæ‰€æœ‰ç±»åˆ«ä¹‹å’Œï¼‰âœ¨ **å¿…å¡«**
- `grammar_total`: è¯­æ³•é”™è¯¯æ€»æ•°ï¼ˆæ‰€æœ‰ç±»åˆ«ä¹‹å’Œï¼‰âœ¨ **å¿…å¡«**
- `total_errors`: æ€»é”™è¯¯æ•°ï¼ˆæ‹¼å†™ + è¯­æ³•ï¼‰âœ¨ **å¿…å¡«**
- `total_words`: æ€»å•è¯æ•°ï¼ˆæ‰€æœ‰è¯„è®ºçš„å•è¯æ€»å’Œï¼‰âœ¨ **å¿…å¡«**
- `errors_per_100_words`: é”™è¯¯ç‡ï¼ˆæ¯100ä¸ªå•è¯çš„é”™è¯¯æ•°ï¼‰âœ¨ **å¿…å¡«**

âš ï¸ **é‡è¦**ï¼šæ¯æ¬¡æ›´æ–°ç»Ÿè®¡æ–‡ä»¶æ—¶ï¼Œå¿…é¡»é‡æ–°è®¡ç®—ä»¥ä¸‹æ±‡æ€»å­—æ®µï¼š
- `spelling_total`ã€`grammar_total`ã€`total_errors`
- `total_words`ï¼ˆéœ€è¦è¯»å–æ‰€æœ‰è¯„è®ºè®¡ç®—å•è¯æ•°ï¼‰
- `errors_per_100_words` = `(total_errors / total_words) * 100`
