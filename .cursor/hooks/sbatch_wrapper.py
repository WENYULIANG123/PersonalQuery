#!/usr/bin/env python3
# Wrapper script to execute Python commands via sbatch and monitor logs

import sys
import os
import re
import subprocess
import time
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any


def parse_command(args):
    """Parse command arguments, handling quoted strings like bash script."""
    if not args:
        return None
    
    if len(args) == 1:
        cmd = args[0]
        # Remove outer quotes if present
        if (cmd.startswith("'") and cmd.endswith("'")) or \
           (cmd.startswith('"') and cmd.endswith('"')):
            return cmd[1:-1]
        return cmd
    else:
        # Combine all arguments
        return ' '.join(args)


def is_job_running(job_id):
    """Check if SLURM job is still running."""
    try:
        result = subprocess.run(
            ['squeue', '-j', str(job_id), '-h'],
            capture_output=True,
            timeout=5,
            text=True
        )
        # If squeue returns empty output (no header), job is not in queue
        # If it has output, job is running
        return bool(result.stdout.strip())
    except (subprocess.TimeoutExpired, subprocess.CalledProcessError):
        # If squeue fails or times out, assume job might still be running
        return True
    except Exception:
        return True


def monitor_logs(log_file, err_file, job_id, max_wait=30):
    """Monitor log files until job completes, checking job status simultaneously."""
    log_path = Path(log_file)
    err_path = Path(err_file)
    
    # Wait for log files to be created (with timeout)
    wait_count = 0
    while wait_count < max_wait:
        if log_path.exists() or err_path.exists():
            break
        # Check job status while waiting
        if not is_job_running(job_id):
            print("[sbatch_wrapper] âš ï¸  ä½œä¸šå·²å®Œæˆä½†æ—¥å¿—æ–‡ä»¶å°šæœªåˆ›å»º", file=sys.stderr)
            return
        time.sleep(1)
        wait_count += 1
    
    # Start tail processes for log and err files separately
    tail_processes = []
    if log_path.exists() or err_path.exists():
        try:
            # Start tail -n +1 -f for log file (output to stdout)
            # -n +1 means start from line 1, -f means follow new lines
            if log_path.exists():
                tail_log = subprocess.Popen(
                    ['tail', '-n', '+1', '-f', '-q', str(log_path)],
                    stdout=sys.stdout,
                    stderr=subprocess.DEVNULL,
                    text=True,
                    bufsize=1
                )
                tail_processes.append(tail_log)
            
            # Start tail -n +1 -f for err file (output to stderr)
            if err_path.exists():
                tail_err = subprocess.Popen(
                    ['tail', '-n', '+1', '-f', '-q', str(err_path)],
                    stdout=sys.stderr,
                    stderr=subprocess.DEVNULL,
                    text=True,
                    bufsize=1
                )
                tail_processes.append(tail_err)
            
            # Monitor until job completes or user interrupts
            # Check job status while tailing logs
            while True:
                # Check if job is still running
                if not is_job_running(job_id):
                    # Job is no longer in queue, wait a bit more for final output
                    time.sleep(0.5)
                    break
                
                # Check if all tail processes are still alive
                if all(p.poll() is not None for p in tail_processes):
                    # All tail processes ended unexpectedly
                    break
                
                time.sleep(0.1)  # Check more frequently
            
            # Give tail processes a moment to flush final output
            time.sleep(0.5)
            
            # Stop all tail processes when job completes
            for tail_process in tail_processes:
                if tail_process and tail_process.poll() is None:
                    tail_process.terminate()
                    try:
                        tail_process.wait(timeout=2)
                    except subprocess.TimeoutExpired:
                        tail_process.kill()
        except Exception as e:
            print(f"[sbatch_wrapper] âš ï¸  ç›‘æ§æ—¥å¿—æ—¶å‡ºé”™: {e}", file=sys.stderr)
            # Clean up any started processes
            for tail_process in tail_processes:
                if tail_process and tail_process.poll() is None:
                    tail_process.terminate()
    else:
        print(
            f"[sbatch_wrapper] âš ï¸  æ—¥å¿—æ–‡ä»¶æœªåœ¨ {max_wait} ç§’å†…åˆ›å»ºï¼Œç»§ç»­ç­‰å¾…ä½œä¸šå®Œæˆ...",
            file=sys.stderr
        )
        # Just wait for job to complete
        while True:
            if not is_job_running(job_id):
                break
            time.sleep(1)


def is_python_script_command(command: str) -> bool:
    """Check if command is executing a Python script."""
    if not command:
        return False
    
    command = command.strip()
    
    # Patterns that indicate Python script execution
    patterns = [
        r'^python\s+.*\.py',           # python script.py
        r'^python3\s+.*\.py',           # python3 script.py
        r'\spython\s+.*\.py',           # ... python script.py
        r'\spython3\s+.*\.py',          # ... python3 script.py
        r'\.py\s',                      # .py file with arguments
        r'\.py$',                       # .py file at end
    ]
    
    for pattern in patterns:
        if re.search(pattern, command, re.IGNORECASE):
            return True
    
    return False


def has_sbatch_in_command(command: str) -> bool:
    """Check if command already contains sbatch or sbatch_wrapper."""
    if not command:
        return False
    
    command_lower = command.lower()
    script_name = "/home/wlia0047/ar57/wenyu/.cursor/hooks/sbatch_wrapper.py"
    
    # Check for various sbatch patterns
    sbatch_patterns = [
        'sbatch',
        'sbatch_wrapper',
        script_name.lower(),
        'sbatch_wrapper.py',
    ]
    
    for pattern in sbatch_patterns:
        if pattern in command_lower or pattern in command:
            return True
    
    return False


def read_input() -> Dict[str, Any]:
    """Read and parse JSON input from stdin (for hook mode)."""
    try:
        if not sys.stdin.isatty():
            raw_input = sys.stdin.read()
            if raw_input.strip():
                return json.loads(raw_input)
    except (json.JSONDecodeError, Exception):
        pass
    return {}


def main():
    # Check if running as a hook (has JSON input from stdin)
    input_data = read_input()
    is_hook_mode = bool(input_data)
    
    if is_hook_mode:
        # Hook mode: intercept command and wrap with sbatch
        command = input_data.get("command", "").strip()
        working_dir = (
            input_data.get("working_directory") or
            input_data.get("cwd") or
            input_data.get("workingDirectory") or
            os.getcwd()
        )
        
        # Debug output
        print(f"[sbatch_wrapper] å‘½ä»¤: {command[:100]}", file=sys.stderr)
        print(f"[sbatch_wrapper] å·¥ä½œç›®å½•: {working_dir}", file=sys.stderr)
        
        # Check if this is a Python script command
        is_python_script = is_python_script_command(command)
        has_sbatch = has_sbatch_in_command(command)
        
        print(f"[sbatch_wrapper] æ˜¯ Python è„šæœ¬å‘½ä»¤: {is_python_script}", file=sys.stderr)
        print(f"[sbatch_wrapper] åŒ…å« sbatch: {has_sbatch}", file=sys.stderr)
        
        # If Python script but no sbatch, block execution
        if is_python_script and not has_sbatch:
            script_path = "/home/wlia0047/ar57/wenyu/.cursor/hooks/sbatch_wrapper.py"
            print("[sbatch_wrapper] âš ï¸  æ£€æµ‹åˆ° Python è„šæœ¬æ‰§è¡Œä½†æœªä½¿ç”¨ sbatch", file=sys.stderr)
            
            output = {
                "continue": True,
                "permission": "deny",
                "user_message": f"Python è„šæœ¬å¿…é¡»ä½¿ç”¨ sbatch æ‰§è¡Œã€‚è¯·ä½¿ç”¨: python3 {script_path} [ä½ çš„å‘½ä»¤]",
                "agent_message": f"æ£€æµ‹åˆ° Python è„šæœ¬æ‰§è¡Œä½†æœªä½¿ç”¨ sbatchã€‚\né‡è¦æç¤ºï¼š\n1. Python è„šæœ¬å¿…é¡»é€šè¿‡ sbatch æ‰§è¡Œ\n2. ä½¿ç”¨å‘½ä»¤: python3 {script_path} {command}\n3. æˆ–è€…ç›´æ¥ä½¿ç”¨: python3 {script_path} \"{command}\""
            }
        elif has_sbatch:
            # Already wrapped, allow as-is
            output = {
                "continue": True,
                "permission": "allow"
            }
        elif command:
            # Not a Python script, allow as-is
            output = {
                "continue": True,
                "permission": "allow"
            }
        else:
            # No command, allow as-is
            output = {
                "continue": True,
                "permission": "allow"
            }
        
        # Output JSON response for hook mode
        print(json.dumps(output, ensure_ascii=False))
        return
    
    # Direct mode: execute command via sbatch
    # Parse command from command line arguments
    original_command = parse_command(sys.argv[1:])
    
    if not original_command:
        print("[sbatch_wrapper] âŒ é”™è¯¯: æœªæä¾›è¦æ‰§è¡Œçš„å‘½ä»¤", file=sys.stderr)
        sys.exit(1)
    
    # Setup paths
    log_dir = Path("/home/wlia0047/ar57/wenyu/logs")
    log_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    log_file = log_dir / f"sbatch_{timestamp}.log"
    err_file = log_dir / f"sbatch_{timestamp}.err"
    script_file = log_dir / f"sbatch_script_{timestamp}.sh"
    
    # Get current directory
    current_dir = os.getcwd() or "/home/wlia0047/ar57/wenyu"
    
    # Create sbatch script
    script_content = f"""#!/bin/bash
#SBATCH --output={log_file}
#SBATCH --error={err_file}
set -e
source /apps/anaconda/2024.02-1/etc/profile.d/conda.sh
conda activate /home/wlia0047/ar57_scratch/wenyu/stark
cd "{current_dir}"
# Execute the original command
{original_command}
"""
    
    script_file.write_text(script_content, encoding='utf-8')
    script_file.chmod(0o755)
    
    # Submit the job
    print("[sbatch_wrapper] æäº¤ä½œä¸šåˆ° SLURM...", file=sys.stderr)
    print(f"[sbatch_wrapper] æ—¥å¿—æ–‡ä»¶: {log_file}", file=sys.stderr)
    print(f"[sbatch_wrapper] é”™è¯¯æ–‡ä»¶: {err_file}", file=sys.stderr)
    
    try:
        result = subprocess.run(
            ['sbatch', str(script_file)],
            capture_output=True,
            text=True,
            check=True
        )
        
        # Extract job ID from output
        match = re.search(r'Submitted batch job (\d+)', result.stdout)
        if not match:
            print("[sbatch_wrapper] âŒ æäº¤ sbatch ä½œä¸šå¤±è´¥: æ— æ³•è§£æä½œä¸š ID", file=sys.stderr)
            sys.exit(1)
        
        job_id = match.group(1)
        
    except subprocess.CalledProcessError as e:
        print(f"[sbatch_wrapper] âŒ æäº¤ sbatch ä½œä¸šå¤±è´¥: {e}", file=sys.stderr)
        if e.stderr:
            print(e.stderr, file=sys.stderr)
        sys.exit(1)
    
    print(f"[sbatch_wrapper] âœ… ä½œä¸šå·²æäº¤ï¼ŒJob ID: {job_id}", file=sys.stderr)
    print(f"[sbatch_wrapper] æ—¥å¿—æ–‡ä»¶: {log_file}", file=sys.stderr)
    print(f"[sbatch_wrapper] é”™è¯¯æ–‡ä»¶: {err_file}", file=sys.stderr)
    print("[sbatch_wrapper] å¼€å§‹ç›‘æ§æ—¥å¿— (Ctrl+C åœæ­¢ç›‘æ§ï¼Œä½œä¸šå°†ç»§ç»­è¿è¡Œ)...", file=sys.stderr)
    
    # Monitor logs
    job_completed = False
    try:
        monitor_logs(str(log_file), str(err_file), job_id)
        job_completed = True
    except KeyboardInterrupt:
        print("\n[sbatch_wrapper] ç›‘æ§å·²åœæ­¢ï¼Œä½œä¸šå°†ç»§ç»­åœ¨åå°è¿è¡Œ...", file=sys.stderr)
    
    print(f"[sbatch_wrapper] âœ… ä½œä¸šå·²å®Œæˆ (Job ID: {job_id})", file=sys.stderr)
    if log_file.exists() or err_file.exists():
        print(
            f"[sbatch_wrapper] æŸ¥çœ‹å®Œæ•´æ—¥å¿—: tail -f {log_file} {err_file}",
            file=sys.stderr
        )
    
    # Clean up temporary files after job completion
    if job_completed:
        cleaned_files = []
        failed_files = []
        
        # Clean up script file
        if script_file.exists():
            try:
                script_file.unlink()
                cleaned_files.append(f"è„šæœ¬æ–‡ä»¶: {script_file.name}")
            except Exception as e:
                failed_files.append(f"è„šæœ¬æ–‡ä»¶: {e}")
        
        # Clean up log file
        if log_file.exists():
            try:
                log_file.unlink()
                cleaned_files.append(f"æ—¥å¿—æ–‡ä»¶: {log_file.name}")
            except Exception as e:
                failed_files.append(f"æ—¥å¿—æ–‡ä»¶: {e}")
        
        # Clean up error file
        if err_file.exists():
            try:
                err_file.unlink()
                cleaned_files.append(f"é”™è¯¯æ–‡ä»¶: {err_file.name}")
            except Exception as e:
                failed_files.append(f"é”™è¯¯æ–‡ä»¶: {e}")
        
        # Print cleanup results
        if cleaned_files:
            print(f"[sbatch_wrapper] ğŸ—‘ï¸  å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {', '.join(cleaned_files)}", file=sys.stderr)
        if failed_files:
            print(f"[sbatch_wrapper] âš ï¸  æ¸…ç†æ–‡ä»¶å¤±è´¥: {', '.join(failed_files)}", file=sys.stderr)


if __name__ == "__main__":
    main()