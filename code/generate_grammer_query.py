import json
import os
import sys
import csv
import time
from typing import List, Dict, Any

# å¯¼å…¥LLMæ¨¡å‹æ¥å£
sys.path.append('/home/wlia0047/ar57/wenyu/stark/code')
from model import get_gm_model

def assess_grammar_dna_from_analysis_file(analysis_file_path):
    """
    ä»ç”¨æˆ·è¯­æ³•åˆ†æJSONæ–‡ä»¶ä¸­åŠ è½½å’Œè§£æç”¨æˆ·çš„è¯­æ³•DNAã€‚

    Args:
        analysis_file_path (str): ç”¨æˆ·è¯­æ³•åˆ†æJSONæ–‡ä»¶çš„è·¯å¾„

    Returns:
        dict: åŒ…å«ç”¨æˆ·è¯­æ³•DNAä¿¡æ¯çš„å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹é”®ï¼š
            - user_id: ç”¨æˆ·ID
            - fingerprint: ç”¨æˆ·è¯­æ³•æŒ‡çº¹æ•°æ®
            - padding_phrases: å¡«å……è¯åˆ—è¡¨
            - interruption_types: æ‰“æ–­ç±»å‹åˆ—è¡¨
            - preferred_generic_nouns: åå¥½çš„é€šç”¨åè¯åˆ—è¡¨
            - analysis_summary: åˆ†ææ‘˜è¦
    """
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(analysis_file_path):
            raise FileNotFoundError(f"User grammar analysis file not found: {analysis_file_path}")

        # è¯»å–å¹¶è§£æJSONæ–‡ä»¶
        with open(analysis_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # éªŒè¯å¿…è¦çš„å­—æ®µ
        if 'user_id' not in data:
            raise ValueError("Missing 'user_id' field in grammar analysis file")
        if 'fingerprint' not in data:
            raise ValueError("Missing 'fingerprint' field in grammar analysis file")

        fingerprint = data['fingerprint']

        # æå–å…³é”®çš„ç”¨æˆ·è¯­æ³•ç‰¹å¾
        user_grammar_dna = {
            'user_id': data['user_id'],
            'fingerprint': fingerprint,
            'padding_phrases': [],
            'interruption_types': [],
            'preferred_generic_nouns': [],
            'analysis_summary': data.get('analysis_summary', {})
        }

        # æå–å¡«å……è¯ï¼ˆpadding phrasesï¼‰
        if 'syntactic' in fingerprint and 'padding_phrases' in fingerprint['syntactic']:
            user_grammar_dna['padding_phrases'] = [
                item['phrase'] for item in fingerprint['syntactic']['padding_phrases']
                if item['frequency'] > 0  # åªåŒ…å«ä½¿ç”¨è¿‡çš„å¡«å……è¯
            ]

        # æå–æ‰“æ–­ç±»å‹ï¼ˆinterruption typesï¼‰
        if 'syntactic' in fingerprint and 'interruption_types' in fingerprint['syntactic']:
            user_grammar_dna['interruption_types'] = [
                item['type'] for item in fingerprint['syntactic']['interruption_types']
                if item['frequency'] > 0  # åªåŒ…å«ä½¿ç”¨è¿‡çš„æ‰“æ–­ç±»å‹
            ]

        # æå–åå¥½çš„é€šç”¨åè¯ï¼ˆpreferred generic nounsï¼‰
        if 'lexical' in fingerprint and 'preferred_generic_nouns' in fingerprint['lexical']:
            user_grammar_dna['preferred_generic_nouns'] = [
                item['noun'] for item in fingerprint['lexical']['preferred_generic_nouns']
                if item['frequency'] > 0  # åªåŒ…å«ä½¿ç”¨è¿‡çš„åè¯
            ]

        print(f"âœ… Successfully loaded user grammar DNA for user: {user_grammar_dna['user_id']}")
        print(f"ğŸ“Š Padding phrases: {len(user_grammar_dna['padding_phrases'])} types")
        print(f"ğŸ“Š Interruption types: {len(user_grammar_dna['interruption_types'])} types")
        print(f"ğŸ“Š Generic nouns: {len(user_grammar_dna['preferred_generic_nouns'])} types")

        return user_grammar_dna

    except json.JSONDecodeError as e:
        print(f"âŒ Error parsing JSON file: {e}")
        raise
    except KeyError as e:
        print(f"âŒ Missing required field in grammar analysis file: {e}")
        raise
    except Exception as e:
        print(f"âŒ Unexpected error loading grammar analysis file: {e}")
        raise


def analyze_query_syntax_for_retrieval_degradation(query_text: str, llm_model, query_id: int = None, verbose: bool = True) -> Dict[str, Any]:
    """
    åˆ†ææŸ¥è¯¢çš„è¯­æ³•ç»“æ„ï¼Œç”Ÿæˆé™ä½æ£€ç´¢æ€§èƒ½çš„æ”¹å†™å»ºè®®ã€‚

    Args:
        query_text (str): åŸå§‹æŸ¥è¯¢æ–‡æœ¬
        llm_model: LLMæ¨¡å‹å®ä¾‹
        query_id (int): æŸ¥è¯¢IDï¼ˆç”¨äºæ—¥å¿—ï¼‰
        verbose (bool): æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—

    Returns:
        dict: åŒ…å«æ”¹å†™å»ºè®®çš„å­—å…¸
    """
    try:
        if verbose:
            print(f"ğŸ¤– Analyzing syntax for query {query_id}: '{query_text}'")

        # æ„é€ prompt - ä¸“é—¨é’ˆå¯¹æ£€ç´¢æ€§èƒ½é™ä½
        prompt = f"""Analyze this search query for syntactic weaknesses affecting retrieval performance on BM25, ANCE, and ColBERTv2.

Query: "{query_text}"

Provide a concise analysis in this format:

## Query Strengths
[List 2-3 key grammatical advantages]

## Key Weaknesses
[Numbered list of 4-6 specific vulnerabilities]

## Impact Summary
| Weakness | BM25 Impact | ANCE Impact | ColBERTv2 Impact |
|----------|-------------|-------------|------------------|
| [Weakness 1] | [1-2 sentence explanation] | [1-2 sentence explanation] | [1-2 sentence explanation] |
| [Weakness 2] | ... | ... | ... |

Keep explanations technical but concise (max 2 sentences per cell). Focus on WHY each weakness hurts performance."""

        # è°ƒç”¨LLM
        start_time = time.time()
        response = llm_model.invoke(prompt)
        response_time = time.time() - start_time

        if verbose:
            print(f"{response_time:.2f}")
            print(f"ğŸ“ Response length: {len(response.content)} characters")
            print(f"ğŸ“ LLM Response content:")
            print(response.content)
            print("="*80)

        # è§£æå“åº”å¹¶æå–ç»“æ„åŒ–ä¿¡æ¯
        try:
            # å°è¯•è§£æLLMå“åº”ï¼Œæå–å…³é”®ä¿¡æ¯
            parsed_analysis = parse_llm_analysis_response(response.content)

            analysis_result = {
                'query_id': query_id,
                'original_query': query_text,
                'llm_response': response.content,
                'parsed_analysis': parsed_analysis,
                'response_time': response_time,
                'timestamp': time.time(),
                'success': True
            }
        except Exception as parse_error:
            print(f"âš ï¸ Failed to parse LLM response: {parse_error}")
            analysis_result = {
                'query_id': query_id,
                'original_query': query_text,
                'llm_response': response.content,
                'parsed_analysis': None,
                'response_time': response_time,
                'timestamp': time.time(),
                'success': True  # ä»ç„¶æ ‡è®°ä¸ºæˆåŠŸï¼Œå› ä¸ºLLMè°ƒç”¨æˆåŠŸäº†
            }

        return analysis_result

    except Exception as e:
        print(f"âŒ Error analyzing query {query_id}: {e}")
        return {
            'query_id': query_id,
            'original_query': query_text,
            'error': str(e),
            'success': False,
            'timestamp': time.time()
        }


def parse_llm_analysis_response(llm_response):
    """
    è§£æLLMç´§å‡‘å¼±ç‚¹åˆ†æå“åº”çš„å†…å®¹ï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯ã€‚

    Args:
        llm_response (str): LLMçš„åŸå§‹å“åº”

    Returns:
        dict: è§£æåçš„ç»“æ„åŒ–æ•°æ®
    """
    try:
        lines = llm_response.strip().split('\n')
        parsed = {
            'query_strengths': [],
            'weaknesses': [],
            'impact_table': []
        }

        current_section = None

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # æ£€æµ‹ç« èŠ‚æ ‡é¢˜
            if line.startswith('## Query Strengths'):
                current_section = 'strengths'
            elif line.startswith('## Key Weaknesses'):
                current_section = 'weaknesses'
            elif line.startswith('## Impact Summary'):
                current_section = 'impact_table'
            elif current_section == 'strengths' and line.startswith('- '):
                # æå–è¯­æ³•ä¼˜åŠ¿
                strength = line.replace('- ', '').strip()
                if strength:
                    parsed['query_strengths'].append(strength)
            elif current_section == 'weaknesses' and line.strip():
                # æå–å¼±ç‚¹ï¼ˆè·³è¿‡æ ‡é¢˜è¡Œï¼‰
                if not line.startswith('##') and not line.startswith('['):
                    # ç§»é™¤ç¼–å·å‰ç¼€ (å¦‚ "1. ", "[1] " ç­‰)
                    weakness = line.lstrip('0123456789.[] ').strip()
                    if weakness and len(weakness) > 5:  # é¿å…ç©ºè¡Œæˆ–è¿‡çŸ­æ–‡æœ¬
                        parsed['weaknesses'].append(weakness)
            elif current_section == 'impact_table' and '|' in line and '---' not in line:
                # è§£æè¡¨æ ¼è¡Œï¼ˆè·³è¿‡åˆ†éš”çº¿ï¼‰
                if 'Weakness' not in line and 'BM25' not in line:  # è·³è¿‡è¡¨å¤´
                    parts = [part.strip() for part in line.split('|') if part.strip()]
                    if len(parts) >= 4:
                        weakness_name = parts[0]
                        bm25_impact = parts[1]
                        ance_impact = parts[2]
                        colbertv2_impact = parts[3]

                        parsed['impact_table'].append({
                            'weakness': weakness_name,
                            'bm25_impact': bm25_impact,
                            'ance_impact': ance_impact,
                            'colbertv2_impact': colbertv2_impact
                        })

        return parsed

    except Exception as e:
        print(f"âš ï¸ Error parsing LLM response: {e}")
        return {
            'raw_response': llm_response,
            'parse_error': str(e)
        }


def generate_user_aware_query_rewrite(original_query: str, retrieval_analysis: dict, user_grammar_dna: dict, llm_model, query_id: int = None, verbose: bool = True) -> Dict[str, Any]:
    """
    åŸºäºæ£€ç´¢åˆ†æå’Œç”¨æˆ·ç”»åƒç”Ÿæˆä¸ªæ€§åŒ–çš„æŸ¥è¯¢æ”¹å†™ã€‚

    Args:
        original_query (str): åŸå§‹æŸ¥è¯¢
        retrieval_analysis (dict): ç¬¬ä¸€æ­¥çš„æ£€ç´¢æ€§èƒ½åˆ†æç»“æœ
        user_grammar_dna (dict): ç”¨æˆ·è¯­æ³•DNA
        llm_model: LLMæ¨¡å‹å®ä¾‹
        query_id (int): æŸ¥è¯¢ID
        verbose (bool): æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—

    Returns:
        dict: åŒ…å«ä¸ªæ€§åŒ–æ”¹å†™ç»“æœçš„å­—å…¸
    """
    try:
        if verbose:
            print(f"ğŸ¨ Generating user-aware rewrite for query {query_id}")

        # æå–ç”¨æˆ·ç”»åƒçš„å…³é”®ç‰¹å¾
        padding_phrases = user_grammar_dna.get('padding_phrases', [])
        generic_nouns = user_grammar_dna.get('preferred_generic_nouns', [])
        interruption_types = user_grammar_dna.get('interruption_types', [])

        # æ„é€ ç»“åˆç”¨æˆ·ç”»åƒçš„prompt
        prompt = f"""Based on the retrieval performance analysis and user grammar profile, create a personalized query rewrite.

**Original Query:** "{original_query}"

**Retrieval Analysis:** {retrieval_analysis.get('llm_response', 'N/A')}

**User Grammar Profile:**
- Preferred padding phrases: {', '.join(padding_phrases)}
- Preferred generic nouns: {', '.join(generic_nouns)}
- Common interruption types: {', '.join(interruption_types)}

**Task:** Create a rewritten query that:
1. Incorporates the user's preferred linguistic patterns (padding phrases, generic nouns)
2. Maintains the core meaning of the original query
3. Follows the user's typical interruption and sentence structure habits
4. May include elements that could affect retrieval performance (based on the analysis)

**Output Format:**
1. **User-Aware Rewrite**: The rewritten query
2. **Incorporated User Patterns**: Which user patterns were used and why
3. **Expected User Reception**: How well this matches the user's writing style
4. **Retrieval Impact Assessment**: How this rewrite might affect BM25/ANCE/ColBERTv2 performance

Make the rewrite sound natural while incorporating the user's linguistic fingerprint."""

        # è°ƒç”¨LLM
        start_time = time.time()
        response = llm_model.invoke(prompt)
        response_time = time.time() - start_time

        if verbose:
            print(f"{response_time:.2f}")
            print(f"ğŸ“ Response length: {len(response.content)} characters")
            print(f"ğŸ“ LLM Response content:")
            print(response.content)
            print("="*80)

        # è§£æå“åº”
        result = {
            'query_id': query_id,
            'original_query': original_query,
            'user_grammar_dna': user_grammar_dna,
            'retrieval_analysis': retrieval_analysis,
            'llm_response': response.content,
            'response_time': response_time,
            'timestamp': time.time(),
            'success': True
        }

        return result

    except Exception as e:
        print(f"âŒ Error in user-aware rewrite for query {query_id}: {e}")
        return {
            'query_id': query_id,
            'original_query': original_query,
            'error': str(e),
            'success': False,
            'timestamp': time.time()
        }


def two_stage_query_rewriting_workflow(queries: List[Dict], user_grammar_dna: dict, llm_model, max_queries: int = None, verbose: bool = True) -> List[Dict]:
    """
    ä¸¤é˜¶æ®µæŸ¥è¯¢æ”¹å†™å·¥ä½œæµï¼šå…ˆåˆ†ææ£€ç´¢å½±å“ï¼Œå†ç»“åˆç”¨æˆ·ç”»åƒæ”¹å†™ã€‚

    Args:
        queries: æŸ¥è¯¢åˆ—è¡¨
        user_grammar_dna: ç”¨æˆ·è¯­æ³•DNA
        llm_model: LLMæ¨¡å‹å®ä¾‹
        max_queries: æœ€å¤§å¤„ç†æŸ¥è¯¢æ•°é‡
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—

    Returns:
        list: å®Œæ•´çš„æ”¹å†™ç»“æœåˆ—è¡¨
    """
    results = []
    total_queries = len(queries) if max_queries is None else min(max_queries, len(queries))

    print(f"\n{'='*100}")
    print(f"ğŸš€ STARTING TWO-STAGE QUERY REWRITING WORKFLOW")
    print(f"{'='*100}")
    print(f"ğŸ“Š Total queries to process: {total_queries}")
    print(f"ğŸ¯ Stage 1: Retrieval Impact Analysis")
    print(f"ğŸ¯ Stage 2: User-Aware Personalized Rewriting")
    print(f"ğŸ¤– Using LLM model: {type(llm_model).__name__}")

    start_time = time.time()
    stage1_success = 0
    stage2_success = 0

    for i, query in enumerate(queries[:total_queries]):
        query_start_time = time.time()

        if verbose:
            print(f"\n{'='*80}")
            print(f"ğŸ” Processing Query {i+1}/{total_queries} (ID: {query['id']})")
            print(f"{'='*80}")

        # Stage 1: æ£€ç´¢æ€§èƒ½åˆ†æ
        if verbose:
            print(f"ğŸ“ˆ Stage 1: Analyzing retrieval impact...")

        retrieval_analysis = analyze_query_syntax_for_retrieval_degradation(
            query_text=query['query'],
            llm_model=llm_model,
            query_id=query['id'],
            verbose=verbose
        )

        if retrieval_analysis['success']:
            stage1_success += 1

            # Stage 2: ç”¨æˆ·ç”»åƒä¸ªæ€§åŒ–æ”¹å†™
            if verbose:
                print(f"ğŸ¨ Stage 2: Generating user-aware rewrite...")

            user_rewrite = generate_user_aware_query_rewrite(
                original_query=query['query'],
                retrieval_analysis=retrieval_analysis,
                user_grammar_dna=user_grammar_dna,
                llm_model=llm_model,
                query_id=query['id'],
                verbose=verbose
            )

            if user_rewrite['success']:
                stage2_success += 1
            else:
                user_rewrite = None

        # ç»„åˆç»“æœ
        combined_result = {
            'query_id': query['id'],
            'original_query': query['query'],
            'stage1_retrieval_analysis': retrieval_analysis,
            'stage2_user_rewrite': user_rewrite,
            'processing_time': time.time() - query_start_time,
            'success': retrieval_analysis['success'] and (user_rewrite['success'] if user_rewrite else False)
        }

        results.append(combined_result)

        # è¿›åº¦æŠ¥å‘Š
        if (i + 1) % 2 == 0:  # æ¯2ä¸ªæŸ¥è¯¢æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
            elapsed = time.time() - start_time
            print(f"\nğŸ“ˆ Progress: {i+1}/{total_queries} queries completed")
            print(f"   Stage 1 Success: {stage1_success}/{i+1}")
            print(f"   Stage 2 Success: {stage2_success}/{stage1_success}")
            print(f"   Elapsed: {elapsed:.2f}s")

    # æœ€ç»ˆç»Ÿè®¡
    total_time = time.time() - start_time
    print(f"\n{'='*100}")
    print(f"ğŸ TWO-STAGE WORKFLOW COMPLETED")
    print(f"{'='*100}")
    print(f"â±ï¸  Total time: {total_time:.2f} seconds")
    print(f"ğŸ“Š Queries processed: {total_queries}")
    print(f"ğŸ“ˆ Stage 1 (Retrieval Analysis): {stage1_success}/{total_queries} successful")
    print(f"ğŸ¨ Stage 2 (User-Aware Rewriting): {stage2_success}/{stage1_success} successful")
    print(".3f")
    if stage1_success > 0:
        print(".2f")

    return results


def batch_analyze_queries_for_retrieval_degradation(queries: List[Dict], llm_model, max_queries: int = None, verbose: bool = True) -> List[Dict]:
    """
    æ‰¹é‡åˆ†ææ‰€æœ‰æŸ¥è¯¢çš„è¯­æ³•æ”¹å†™å»ºè®®ã€‚

    Args:
        queries: æŸ¥è¯¢åˆ—è¡¨
        llm_model: LLMæ¨¡å‹å®ä¾‹
        max_queries: æœ€å¤§å¤„ç†æŸ¥è¯¢æ•°é‡ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—

    Returns:
        list: åˆ†æç»“æœåˆ—è¡¨
    """
    results = []
    total_queries = len(queries) if max_queries is None else min(max_queries, len(queries))

    print(f"\n{'='*80}")
    print(f"ğŸš€ STARTING BATCH SYNTAX ANALYSIS FOR RETRIEVAL DEGRADATION")
    print(f"{'='*80}")
    print(f"ğŸ“Š Total queries to analyze: {total_queries}")
    print(f"ğŸ¯ Target: Reduce BM25/ANCE/ColBERTv2 retrieval performance")
    print(f"ğŸ¤– Using LLM model: {type(llm_model).__name__}")

    start_time = time.time()
    successful_analyses = 0
    failed_analyses = 0

    for i, query in enumerate(queries[:total_queries]):
        query_start_time = time.time()

        if verbose:
            print(f"\n{'-'*60}")
            print(f"ğŸ” Processing Query {i+1}/{total_queries} (ID: {query['id']})")
            print(f"{'-'*60}")

        # åˆ†ææŸ¥è¯¢
        result = analyze_query_syntax_for_retrieval_degradation(
            query_text=query['query'],
            llm_model=llm_model,
            query_id=query['id'],
            verbose=verbose
        )

        # è®°å½•ç»“æœ
        results.append(result)

        if result['success']:
            successful_analyses += 1
        else:
            failed_analyses += 1

        query_time = time.time() - query_start_time
        if verbose:
            status = "âœ…" if result['success'] else "âŒ"
            print(f"{status} {query_time:.2f}s")
        # æ¯å¤„ç†10ä¸ªæŸ¥è¯¢æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
        if (i + 1) % 10 == 0:
            elapsed = time.time() - start_time
            print(f"\nğŸ“ˆ Progress: {i+1}/{total_queries} queries processed")
            print(f"â±ï¸ Elapsed: {elapsed:.2f}s")
    # æœ€ç»ˆç»Ÿè®¡
    total_time = time.time() - start_time
    print(f"\n{'='*80}")
    print(f"ğŸ BATCH ANALYSIS COMPLETED")
    print(f"{'='*80}")
    print(f"â±ï¸  Total time: {total_time:.2f} seconds")
    print(f"ğŸ“Š Queries processed: {total_queries}")
    print(f"âœ… Successful analyses: {successful_analyses}")
    print(f"âŒ Failed analyses: {failed_analyses}")
    print(".3f")
    if successful_analyses > 0:
        print(".2f")
    return results


def load_human_generated_queries(csv_file_path):
    """
    ä»CSVæ–‡ä»¶ä¸­åŠ è½½äººå·¥ç”Ÿæˆçš„æŸ¥è¯¢æ•°æ®ã€‚

    Args:
        csv_file_path (str): CSVæ–‡ä»¶è·¯å¾„

    Returns:
        list: åŒ…å«æŸ¥è¯¢æ•°æ®çš„å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«ï¼š
            - id: æŸ¥è¯¢ID
            - query: æŸ¥è¯¢æ–‡æœ¬
            - answer_ids: ç›¸å…³ç­”æ¡ˆIDåˆ—è¡¨
            - answer_ids_source: æ¥æºç­”æ¡ˆIDåˆ—è¡¨
    """
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(csv_file_path):
            raise FileNotFoundError(f"Query CSV file not found: {csv_file_path}")

        queries = []

        # è¯»å–CSVæ–‡ä»¶
        with open(csv_file_path, 'r', encoding='utf-8') as csvfile:
            # ä½¿ç”¨csv.DictReaderè‡ªåŠ¨å¤„ç†å­—æ®µå
            reader = csv.DictReader(csvfile)

            for row_num, row in enumerate(reader, 1):
                try:
                    # éªŒè¯å¿…éœ€å­—æ®µ
                    if 'id' not in row or 'query' not in row:
                        raise ValueError(f"Missing required fields 'id' or 'query' in row {row_num}")

                    # è§£æanswer_ids (JSONæ ¼å¼çš„åˆ—è¡¨)
                    answer_ids = []
                    if row.get('answer_ids'):
                        try:
                            # ç§»é™¤å¯èƒ½çš„å¼•å·å¹¶è§£æJSON
                            answer_ids_str = row['answer_ids'].strip('"')
                            answer_ids = json.loads(answer_ids_str)
                        except json.JSONDecodeError:
                            print(f"âš ï¸ Warning: Could not parse answer_ids in row {row_num}: {row['answer_ids']}")

                    # è§£æanswer_ids_source (JSONæ ¼å¼çš„åˆ—è¡¨)
                    answer_ids_source = []
                    if row.get('answer_ids_source'):
                        try:
                            # ç§»é™¤å¯èƒ½çš„å¼•å·å¹¶è§£æJSON
                            source_str = row['answer_ids_source'].strip('"')
                            answer_ids_source = json.loads(source_str)
                        except json.JSONDecodeError:
                            print(f"âš ï¸ Warning: Could not parse answer_ids_source in row {row_num}: {row['answer_ids_source']}")

                    # åˆ›å»ºæŸ¥è¯¢å­—å…¸
                    query_data = {
                        'id': int(row['id']),
                        'query': row['query'].strip(),
                        'answer_ids': answer_ids,
                        'answer_ids_source': answer_ids_source,
                        'row_number': row_num
                    }

                    queries.append(query_data)

                except Exception as e:
                    print(f"âŒ Error processing row {row_num}: {e}")
                    continue

        print(f"âœ… Successfully loaded {len(queries)} queries from CSV file")
        print(f"ğŸ“Š Total rows processed: {row_num}")

        # ç»Ÿè®¡ä¿¡æ¯
        total_answer_ids = sum(len(q['answer_ids']) for q in queries)
        total_source_ids = sum(len(q['answer_ids_source']) for q in queries)
        avg_query_length = sum(len(q['query'].split()) for q in queries) / len(queries) if queries else 0

        print(f"ğŸ“Š Average query length: {avg_query_length:.1f} words")
        print(f"ğŸ“Š Total answer IDs: {total_answer_ids}")
        print(f"ğŸ“Š Total source answer IDs: {total_source_ids}")

        return queries

    except Exception as e:
        print(f"âŒ Unexpected error loading CSV file: {e}")
        raise


def print_query_details(queries, max_display=10):
    """æ‰“å°æŸ¥è¯¢æ•°æ®çš„è¯¦ç»†ä¿¡æ¯"""
    print(f"\n{'='*100}")
    print(f"ğŸ“‹ QUERY DATA DETAILS (showing first {min(max_display, len(queries))} queries)")
    print(f"{'='*100}")

    for i, query in enumerate(queries[:max_display]):
        print(f"\nğŸ” Query #{query['id']} (Row {query['row_number']}):")
        print(f"   ğŸ“ Text: {query['query']}")
        print(f"   ğŸ¯ Answer IDs: {len(query['answer_ids'])} items - {query['answer_ids'][:5]}{'...' if len(query['answer_ids']) > 5 else ''}")
        print(f"   ğŸ“ Source IDs: {len(query['answer_ids_source'])} items - {query['answer_ids_source']}")

        # åˆ†ææŸ¥è¯¢é•¿åº¦
        word_count = len(query['query'].split())
        char_count = len(query['query'])
        print(f"   ğŸ“Š Length: {word_count} words, {char_count} characters")

    if len(queries) > max_display:
        print(f"\n... and {len(queries) - max_display} more queries")

    print(f"\n{'='*100}")


def print_user_profile_details(user_grammar_dna):
    """è¯¦ç»†æ‰“å°ç”¨æˆ·ç”»åƒçš„æ‰€æœ‰å†…å®¹"""
    fingerprint = user_grammar_dna['fingerprint']

    print(f"\n{'='*100}")
    print(f"ğŸ“‹ DETAILED USER PROFILE CONTENTS")
    print(f"{'='*100}")

    # åŸºæœ¬ä¿¡æ¯
    print(f"ğŸ‘¤ User ID: {user_grammar_dna['user_id']}")

    # è¯­æ³•ç‰¹å¾ (Syntactic)
    if 'syntactic' in fingerprint:
        print(f"\nğŸ”¤ SYNTACTIC FEATURES:")
        print(f"{'-'*30}")

        # Padding Phrases
        if 'padding_phrases' in fingerprint['syntactic']:
            print(f"ğŸ“ Padding Phrases ({len(fingerprint['syntactic']['padding_phrases'])} types):")
            for item in fingerprint['syntactic']['padding_phrases']:
                print(f"   â€¢ '{item['phrase']}' (frequency: {item['frequency']})")

        # Interruption Types
        if 'interruption_types' in fingerprint['syntactic']:
            print(f"\nğŸ”„ Interruption Types ({len(fingerprint['syntactic']['interruption_types'])} types):")
            for item in fingerprint['syntactic']['interruption_types']:
                print(f"   â€¢ {item['type']} (frequency: {item['frequency']})")

        # Conditional Usage Levels
        if 'conditional_usage_levels' in fingerprint['syntactic']:
            print(f"\nğŸ”€ Conditional Usage Levels ({len(fingerprint['syntactic']['conditional_usage_levels'])} levels):")
            for item in fingerprint['syntactic']['conditional_usage_levels']:
                print(f"   â€¢ {item['level']} (frequency: {item['frequency']})")

    # è¯æ±‡ç‰¹å¾ (Lexical)
    if 'lexical' in fingerprint:
        print(f"\nğŸ“Š LEXICAL FEATURES:")
        print(f"{'-'*30}")

        # Circumlocution Habits
        if 'circumlocution_habits' in fingerprint['lexical']:
            print(f"ğŸ”„ Circumlocution Habits ({len(fingerprint['lexical']['circumlocution_habits'])} types):")
            for item in fingerprint['lexical']['circumlocution_habits']:
                print(f"   â€¢ {item['type']} (frequency: {item['frequency']})")

        # Preferred Generic Nouns
        if 'preferred_generic_nouns' in fingerprint['lexical']:
            print(f"\nğŸ“Š Preferred Generic Nouns ({len(fingerprint['lexical']['preferred_generic_nouns'])} nouns):")
            for item in fingerprint['lexical']['preferred_generic_nouns']:
                print(f"   â€¢ '{item['noun']}' (frequency: {item['frequency']})")

    # é€»è¾‘ç‰¹å¾ (Logic)
    if 'logic' in fingerprint:
        print(f"\nğŸ§  LOGIC FEATURES:")
        print(f"{'-'*30}")

        # Past Reference Habits
        if 'past_reference_habits' in fingerprint['logic']:
            print(f"â° Past Reference Habits ({len(fingerprint['logic']['past_reference_habits'])} levels):")
            for item in fingerprint['logic']['past_reference_habits']:
                print(f"   â€¢ {item['level']} (frequency: {item['frequency']})")

        # Negation Styles
        if 'negation_styles' in fingerprint['logic']:
            print(f"\nâŒ Negation Styles ({len(fingerprint['logic']['negation_styles'])} styles):")
            for item in fingerprint['logic']['negation_styles']:
                print(f"   â€¢ {item['type']} (frequency: {item['frequency']})")

    # ä¸Šä¸‹æ–‡æ´å¯Ÿ (Contextual Insights)
    if 'contextual_insights' in fingerprint:
        print(f"\nğŸ’¡ CONTEXTUAL INSIGHTS:")
        print(f"{'-'*30}")

        insights = fingerprint['contextual_insights']
        if 'syntactic' in insights:
            print("ğŸ”¤ Syntactic Insights:")
            for key, value in insights['syntactic'].items():
                print(f"   â€¢ {key}: {value}")

        if 'lexical' in insights:
            print("\nğŸ“Š Lexical Insights:")
            for key, value in insights['lexical'].items():
                print(f"   â€¢ {key}: {value}")

        if 'logic' in insights:
            print("\nğŸ§  Logic Insights:")
            for key, value in insights['logic'].items():
                print(f"   â€¢ {key}: {value}")

    # é£æ ¼æ€»ç»“å’Œåˆ†ææ‘˜è¦
    if 'style_summary' in fingerprint:
        print(f"\nâœ¨ STYLE SUMMARY:")
        print(f"{'-'*30}")
        print(f"{fingerprint['style_summary']}")

    if 'analysis_summary' in fingerprint:
        print(f"\nğŸ“ˆ ANALYSIS SUMMARY:")
        print(f"{'-'*30}")
        summary = fingerprint['analysis_summary']
        if 'total_sentences_analyzed' in summary:
            print(f"ğŸ“Š Total Sentences Analyzed: {summary['total_sentences_analyzed']}")

        if 'dimension_percentages' in summary:
            print("ğŸ“Š Dimension Percentages:")
            percentages = summary['dimension_percentages']
            for key, value in percentages.items():
                print(f"   â€¢ {key}: {value:.2f}%")

        if 'raw_counts' in summary:
            print("\nğŸ“Š Raw Counts:")
            raw_counts = summary['raw_counts']
            for key, value in raw_counts.items():
                print(f"   â€¢ {key}: {value}")

    print(f"\n{'='*100}")


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç”¨æˆ·ç”»åƒå’ŒæŸ¥è¯¢æ•°æ®åŠ è½½åŠŸèƒ½"""
    import time

    print("ğŸš€ Starting main function...")
    start_time = time.time()
    print(f"ğŸš€ [{time.time() - start_time:.1f}s] Starting User Profile & Query Data Loading Demo...")

    # æ–‡ä»¶è·¯å¾„
    analysis_file_path = "/home/wlia0047/ar57_scratch/wenyu/amazon_review_grammer_analysis.json"
    csv_file_path = "/home/wlia0047/ar57/wenyu/stark/data/stark_qa_human_generated_eval.csv"

    try:
        # ç¬¬ä¸€æ­¥ï¼šåŠ è½½ç”¨æˆ·è¯­æ³•DNA
        print(f"\n{'='*60}")
        print(f"ğŸ¯ STEP 1: Loading User Grammar DNA")
        print(f"{'='*60}")
        print(f"ğŸ“‹ [{time.time() - start_time:.1f}s] Loading user grammar analysis file...")
        user_grammar_dna = assess_grammar_dna_from_analysis_file(analysis_file_path)

        # ç¬¬äºŒæ­¥ï¼šåŠ è½½æŸ¥è¯¢æ•°æ®
        print(f"\n{'='*60}")
        print(f"ğŸ¯ STEP 2: Loading Human Generated Queries")
        print(f"{'='*60}")
        print(f"ğŸ“„ [{time.time() - start_time:.1f}s] Loading query CSV file...")
        queries = load_human_generated_queries(csv_file_path)

        # æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ
        print(f"\n{'='*80}")
        print(f"ğŸ“Š DATA LOADING SUMMARY")
        print(f"{'='*80}")
        print(f"ğŸ‘¤ User Profile: {user_grammar_dna['user_id']}")
        print(f"ğŸ“ Queries Loaded: {len(queries)} items")
        print(f"ğŸ“ˆ Analysis Sentences: {user_grammar_dna['fingerprint']['analysis_summary']['total_sentences_analyzed']}")

        # è¯¦ç»†æ‰“å°å‰å‡ ä¸ªæŸ¥è¯¢
        print_query_details(queries, max_display=3)

        # ç¬¬ä¸‰æ­¥ï¼šä¸¤é˜¶æ®µæŸ¥è¯¢æ”¹å†™å·¥ä½œæµ
        print(f"\n{'='*60}")
        print(f"ğŸ¯ STEP 3: Two-Stage Query Rewriting Workflow")
        print(f"{'='*60}")
        print(f"ğŸ“ˆ Stage 3.1: Retrieval Impact Analysis")
        print(f"ğŸ¨ Stage 3.2: User-Aware Personalized Rewriting")

        # åˆå§‹åŒ–LLMæ¨¡å‹
        print(f"ğŸ¤– [{time.time() - start_time:.1f}s] Initializing LLM model...")
        try:
            llm_model = get_gm_model()
            print(f"âœ… [{time.time() - start_time:.1f}s] LLM model initialized successfully")
        except Exception as e:
            print(f"âŒ [{time.time() - start_time:.1f}s] Failed to initialize LLM: {e}")
            print(f"âš ï¸  Continuing with data loading only...")
            llm_model = None

        # æ‰§è¡Œç¬¬ä¸€é˜¶æ®µæ£€ç´¢å½±å“åˆ†æï¼ˆåªåˆ†æå‰3ä¸ªæŸ¥è¯¢ç”¨äºæ¼”ç¤ºï¼‰
        if llm_model:
            print(f"ğŸ”¬ [{time.time() - start_time:.1f}s] Starting retrieval impact analysis...")
            analysis_results = batch_analyze_queries_for_retrieval_degradation(
                queries=queries,
                llm_model=llm_model,
                max_queries=3,  # æ¼”ç¤ºç”¨ï¼Œåªå¤„ç†å‰3ä¸ª
                verbose=True
            )

            print(f"\nğŸ“‹ [{time.time() - start_time:.1f}s] Retrieval impact analysis completed for {len(analysis_results)} queries")

            # å±•ç¤ºåˆ†æç»“æœ
            display_analysis_results(analysis_results, max_display=3)

            # ä¿å­˜åˆ†æç»“æœ
            output_file = "/home/wlia0047/ar57_scratch/wenyu/syntax_weakness_analysis_results.json"
            save_analysis_results_to_file(analysis_results, output_file)

        else:
            analysis_results = []
            print(f"âš ï¸  Skipping analysis due to LLM initialization failure")

        # æœ€ç»ˆæ€»ç»“
        print(f"\n{'='*100}")
        print(f"ğŸ‰ COMPLETE ANALYSIS EXECUTION SUMMARY")
        print(f"{'='*100}")
        print(f"âœ… Step 1 - User Profile: LOADED ({user_grammar_dna['user_id']})")
        print(f"âœ… Step 2 - Queries: LOADED ({len(queries)} items)")
        if analysis_results:
            successful_analyses = sum(1 for r in analysis_results if r['success'])
            print(f"âœ… Step 3 - Retrieval Impact Analysis: COMPLETED")
            print(f"   ğŸ“ˆ Successful Analyses: {successful_analyses}/{len(analysis_results)}")
        else:
            print(f"âŒ Step 3 - Retrieval Impact Analysis: SKIPPED (LLM unavailable)")

        print(f"\nâ±ï¸  Total execution time: {time.time() - start_time:.2f} seconds")
        print(f"ğŸ“‹ Grammar file: {analysis_file_path}")
        print(f"ğŸ“„ Query file: {csv_file_path}")

        # è¿”å›å®Œæ•´ç»“æœ
        result = {
            'user_grammar_dna': user_grammar_dna,
            'queries': queries,
            'analysis_results': analysis_results,
            'metadata': {
                'grammar_file': analysis_file_path,
                'query_file': csv_file_path,
                'queries_loaded': len(queries),
                'analyses_completed': len(analysis_results),
                'user_id': user_grammar_dna['user_id'],
                'execution_time': time.time() - start_time
            }
        }

        return result

    except Exception as e:
        print(f"âŒ [{time.time() - start_time:.1f}s] Failed to load data: {e}")
        sys.exit(1)


def display_analysis_results(analysis_results, max_display=5):
    """
    æ ¼å¼åŒ–å±•ç¤ºç´§å‡‘çš„åˆ†æç»“æœã€‚

    Args:
        analysis_results: åˆ†æç»“æœåˆ—è¡¨
        max_display: æœ€å¤šæ˜¾ç¤ºå¤šå°‘ä¸ªç»“æœ
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“Š SYNTAX WEAKNESS ANALYSIS")
    print(f"{'='*80}")

    successful_analyses = [r for r in analysis_results if r.get('success', False)]
    print(f"ğŸ“ˆ Total: {len(analysis_results)} | âœ… Success: {len(successful_analyses)}")

    for i, result in enumerate(successful_analyses[:max_display]):
        print(f"\n{'â”€'*70}")
        print(f"ğŸ” Query {i+1} (ID: {result.get('query_id', 'N/A')})")
        print(f"ğŸ“ {result.get('original_query', 'N/A')}")

        parsed = result.get('parsed_analysis', {})
        if parsed:
            # è¯­æ³•ä¼˜åŠ¿
            if parsed.get('query_strengths'):
                print(f"\nğŸ’ª Strengths:")
                for strength in parsed['query_strengths'][:2]:  # æœ€å¤šæ˜¾ç¤º2ä¸ª
                    print(f"   â€¢ {strength}")

            # å…³é”®å¼±ç‚¹
            if parsed.get('weaknesses'):
                print(f"\nâš ï¸ Weaknesses ({len(parsed['weaknesses'])}):")
                for j, weakness in enumerate(parsed['weaknesses'], 1):
                    print(f"   {j}. {weakness}")

            # å½±å“è¡¨æ ¼
            if parsed.get('impact_table'):
                print(f"\nğŸ“‹ Impact Summary:")
                print(f"   {'Weakness':<25} {'BM25':<8} {'ANCE':<8} {'ColBERTv2'}")
                print(f"   {'â”€'*25} {'â”€'*8} {'â”€'*8} {'â”€'*10}")
                for row in parsed['impact_table'][:4]:  # æœ€å¤šæ˜¾ç¤º4ä¸ª
                    weakness_short = row['weakness'][:24] + "..." if len(row['weakness']) > 24 else row['weakness']
                    bm25_short = row['bm25_impact'][:7] + "..." if len(row['bm25_impact']) > 7 else row['bm25_impact']
                    ance_short = row['ance_impact'][:7] + "..." if len(row['ance_impact']) > 7 else row['ance_impact']
                    colbert_short = row['colbertv2_impact'][:9] + "..." if len(row['colbertv2_impact']) > 9 else row['colbertv2_impact']
                    print(f"   {weakness_short:<25} {bm25_short:<8} {ance_short:<8} {colbert_short}")

        print(f"\nâ±ï¸ {result.get('response_time', 0):.2f}s")

    if len(successful_analyses) > max_display:
        print(f"\n... and {len(successful_analyses) - max_display} more analyses")

    print(f"\n{'='*80}")


def save_analysis_results_to_file(analysis_results, output_file_path):
    """
    å°†åˆ†æç»“æœä¿å­˜ä¸ºJSONæ–‡ä»¶ã€‚

    Args:
        analysis_results: åˆ†æç»“æœåˆ—è¡¨
        output_file_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    try:
        # å‡†å¤‡ä¿å­˜çš„æ•°æ®ç»“æ„
        save_data = {
            'metadata': {
                'total_analyses': len(analysis_results),
                'successful_analyses': sum(1 for r in analysis_results if r.get('success', False)),
                'timestamp': int(time.time()),
                'analysis_type': 'syntax_weakness_analysis'
            },
            'results': analysis_results
        }

        # ä¿å­˜ä¸ºJSONæ–‡ä»¶
        with open(output_file_path, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ Analysis results saved to: {output_file_path}")
        print(f"ğŸ“Š Total results: {save_data['metadata']['total_analyses']}")
        print(f"âœ… Successful: {save_data['metadata']['successful_analyses']}")

    except Exception as e:
        print(f"âŒ Error saving analysis results: {e}")


if __name__ == "__main__":
    main()
