#!/usr/bin/env python3
"""
ç”¨æˆ·åå¥½å®ä½“æå–æ¨¡å—
è´Ÿè´£å¤„ç†ç”¨æˆ·åå¥½å®ä½“çš„æå–å’Œå¤„ç†
"""

import os
import json
import gzip
import sys
import re
from typing import Dict, List, Optional, Union, Tuple
from datetime import datetime
from collections import defaultdict

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from model import call_llm_with_retry, APIErrorException

def log_with_timestamp(message: str):
    """Log message with timestamp."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {message}", flush=True)

def normalize_category_label(category: str) -> str:
    """
    Normalize category labels to keep keys consistent across pipeline.
    Currently enforces: "Color/Finish" -> "Color" (and common variants).
    """
    if category is None:
        return category
    c = str(category).strip()
    if not c:
        return c
    c_lower = c.lower().strip()
    c_compact = c_lower.replace(" ", "")
    if c_compact in {"color/finish", "colour/finish", "colorfinish", "colourfinish"}:
        return "Color"
    if c_lower in {"color", "colour"}:
        return "Color"
    return c

def clean_html_content(text) -> str:
    """Remove HTML tags and clean up content for entity extraction."""
    if not text:
        return ""

    # Convert to string if it's not already
    if not isinstance(text, str):
        text = str(text)

    # Remove HTML tags
    text = re.sub(r'<[^>]+>', '', text)

    # Remove JavaScript content
    text = re.sub(r'javascript:[^\'"\\s]*', '', text)

    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text)

    # Remove common Amazon UI text patterns
    text = re.sub(r'Save \d+% on.*?(?=when|$)', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Enter code [A-Z0-9]+ at checkout', '', text, flags=re.IGNORECASE)
    text = re.sub(r'restrictions apply', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Here\'s how', '', text, flags=re.IGNORECASE)

    # Clean up extra spaces again
    text = re.sub(r'\s+', ' ', text).strip()

    return text

def load_data_from_gzip(file_path: str, data_type: str, filter_func=None, max_items: int = None) -> Union[List[Dict], Dict[str, Dict]]:
    """ä»gzipæ–‡ä»¶åŠ è½½æ•°æ®"""
    try:
        with gzip.open(file_path, 'rt', encoding='utf-8') as f:
            if data_type == 'metadata':
                # Load metadata as dict keyed by asin
                data = {}
                for line_num, line in enumerate(f):
                    if max_items and line_num >= max_items:
                        break
                    try:
                        item = json.loads(line.strip())
                        asin = item.get('asin', '')
                        if asin:
                            if filter_func is None or filter_func(item):
                                data[asin] = item
                    except json.JSONDecodeError:
                        continue
                return data
            else:
                # Load reviews as list
                data = []
                for line_num, line in enumerate(f):
                    if max_items and line_num >= max_items:
                        break
                    try:
                        item = json.loads(line.strip())
                        if filter_func is None or filter_func(item):
                            data.append(item)
                    except json.JSONDecodeError:
                        continue
                return data
    except Exception as e:
        log_with_timestamp(f"Error loading {data_type} from {file_path}: {e}")
        return {} if data_type == 'metadata' else []

def load_data(data_type: str, filter_func=None, max_items: int = None, user_products: set = None):
    """åŠ è½½æ•°æ®çš„é€šç”¨å‡½æ•°"""
    if data_type == 'metadata':
        file_path = "/home/wlia0047/ar57/wenyu/data/Amazon-Reviews-2018/raw/meta_Arts_Crafts_and_Sewing.json.gz"
    elif data_type == 'reviews':
        file_path = "/home/wlia0047/ar57/wenyu/data/Amazon-Reviews-2018/raw/Arts_Crafts_and_Sewing.json.gz"
    else:
        raise ValueError(f"Unknown data type: {data_type}")

    return load_data_from_gzip(file_path, data_type, filter_func, max_items)

TARGET_USER = "AG7EF0SVBQOUX"
REVIEW_FILE = "/home/wlia0047/ar57/wenyu/data/Amazon-Reviews-2018/raw/Arts_Crafts_and_Sewing.json.gz"
META_FILE = "/home/wlia0047/ar57/wenyu/data/Amazon-Reviews-2018/raw/meta_Arts_Crafts_and_Sewing.json.gz"
OUTPUT_FILE = "/home/wlia0047/ar57_scratch/wenyu/user_preference_queries.json"

def load_user_reviews(target_user: str = None) -> List[Dict]:
    """åŠ è½½æŒ‡å®šç”¨æˆ·çš„æ‰€æœ‰è¯„è®ºï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    if target_user:
        def filter_func(data):
            user_id = data.get('user_id') or data.get('reviewerID') or data.get('reviewer_id')
            return user_id == target_user
        return load_data('reviews', filter_func, max_items=None)  # Remove max_items limit when filtering for specific user
    return load_data('reviews', max_items=100)

def process_user_preference_extraction_response(response_str: str) -> tuple:
    """
    å¤„ç†ç”¨æˆ·åå¥½å®ä½“æå–çš„LLMå“åº”

    Args:
        response_str: LLMè¿”å›çš„åŸå§‹å­—ç¬¦ä¸²

    Returns:
        (flattened_entities_list, categorized_entities_dict) å…ƒç»„

    Raises:
        APIErrorException: å½“å“åº”æ— æ•ˆæˆ–æ— æ³•è§£ææ—¶
    """
    if not response_str:
        raise APIErrorException("No response from user preference extraction")

    try:
        def _normalize_sentiment(val: Optional[str]) -> str:
            if not val:
                return "neutral"
            v = str(val).strip().lower()
            if v in {"positive", "pos", "+"}:
                return "positive"
            if v in {"negative", "neg", "-"}:
                return "negative"
            if v in {"neutral", "neu", "unknown", "mixed"}:
                return "neutral"
            return "neutral"

        def _coerce_entity_item(item) -> Optional[Dict[str, str]]:
            """
            Normalize entity item to: {"entity": <str>, "sentiment": <positive|negative|neutral>}
            Supports:
              - "string"
              - {"entity": "...", "sentiment": "..."} (also accepts keys: text/name, polarity)
            """
            if isinstance(item, str):
                entity_text = item.strip()
                if not entity_text:
                    return None
                return {"entity": entity_text, "sentiment": "neutral"}

            if isinstance(item, dict):
                entity_text = (
                    item.get("entity")
                    or item.get("text")
                    or item.get("name")
                    or ""
                )
                entity_text = str(entity_text).strip()
                if not entity_text:
                    return None
                sentiment = _normalize_sentiment(item.get("sentiment") or item.get("polarity"))
                return {"entity": entity_text, "sentiment": sentiment}

            return None

        # Clean the response
        response_str = response_str.strip()

        # Smart JSON extraction for Chain of Thought responses
        lines = response_str.strip().split('\n')
        json_found = False

        # Check if the last few lines contain valid JSON
        for i in range(len(lines) - 1, max(-1, len(lines) - 5), -1):  # Check last 5 lines
            line = lines[i].strip()
            if line.startswith('{') and line.endswith('}'):
                # Found JSON object at the end
                response_str = line
                json_found = True
                break

        # If no JSON found at the end, look for code blocks
        if not json_found:
            # Find the LAST json code block (in case there are multiple)
            json_blocks = []
            start = 0
            while True:
                json_start = response_str.find('```json', start)
                if json_start == -1:
                    break
                json_end = response_str.find('```', json_start + 7)
                if json_end == -1:
                    break
                content_start = response_str.find('\n', json_start) + 1
                if content_start > 0:
                    content = response_str[content_start:json_end].strip()
                    if content:
                        json_blocks.append(content)
                start = json_end + 3

            # Also handle regular ``` blocks
            if not json_blocks:
                if '```' in response_str:
                    # Find the LAST code block
                    last_triple = response_str.rfind('```')
                    first_triple = response_str.rfind('```', 0, last_triple)
                    if first_triple != last_triple:
                        content_start = response_str.find('\n', first_triple) + 1
                        if content_start > 0:
                            response_str = response_str[content_start:last_triple].strip()
                    else:
                        # Single code block
                        content_start = response_str.find('\n', first_triple) + 1
                        if content_start > 0:
                            response_str = response_str[content_start:].strip()
            
            # Use the last json block if found
            if json_blocks:
                response_str = json_blocks[-1]

        # Try to parse as JSON
        result = json.loads(response_str)

        # Handle different response formats
        if isinstance(result, dict):
            # New format:
            #  - {category: ["entity", ...]}
            #  - {category: [{"entity": "...", "sentiment": "positive|negative|neutral"}, ...]}
            categorized_entities = {}
            flattened = []

            for category, entities in result.items():
                category_norm = normalize_category_label(category)
                if isinstance(entities, list):
                    # Process list of entities for this category
                    category_entities = []
                    for entity in entities:
                        normalized = _coerce_entity_item(entity)
                        if not normalized:
                            continue
                        entity_text = normalized["entity"]

                        # Apply atomic filtering on entity text
                        entity_lower = entity_text.lower()
                        if (',' in entity_text or
                                ' and ' in entity_lower or
                                ' with ' in entity_lower or
                                ' or ' in entity_lower or
                                ' for ' in entity_lower or
                                '&' in entity_text):
                            continue

                        category_entities.append(normalized)
                        flattened.append(entity_text)

                    if category_entities:
                        if category_norm not in categorized_entities:
                            categorized_entities[category_norm] = []
                        categorized_entities[category_norm].extend(category_entities)
                elif isinstance(entities, str):
                    # Handle single entity as string (legacy)
                    normalized = _coerce_entity_item(entities)
                    if normalized:
                        entity_text = normalized["entity"]
                        entity_lower = entity_text.lower()
                        if not (',' in entity_text or
                                ' and ' in entity_lower or
                                ' with ' in entity_lower or
                                ' or ' in entity_lower or
                                ' for ' in entity_lower or
                                '&' in entity_text):
                            categorized_entities[category_norm] = [normalized]
                            flattened.append(entity_text)
                elif isinstance(entities, dict):
                    # Handle single entity object
                    normalized = _coerce_entity_item(entities)
                    if normalized:
                        entity_text = normalized["entity"]
                        entity_lower = entity_text.lower()
                        if not (',' in entity_text or
                                ' and ' in entity_lower or
                                ' with ' in entity_lower or
                                ' or ' in entity_lower or
                                ' for ' in entity_lower or
                                '&' in entity_text):
                            categorized_entities[category_norm] = [normalized]
                            flattened.append(entity_text)

            if flattened:
                return flattened, categorized_entities
            else:
                raise APIErrorException("No valid entities extracted from user preference extraction")

        elif isinstance(result, list):
            # Legacy format: array of strings - convert to new format with neutral sentiment
            categorized_entities = {"General": []}
            flattened = []

            for item in result:
                normalized = _coerce_entity_item(item)
                if not normalized:
                    continue
                entity_text = normalized["entity"]
                # Apply atomic filtering
                entity_lower = entity_text.lower()
                if (',' in entity_text or
                        ' and ' in entity_lower or
                        ' with ' in entity_lower or
                        ' or ' in entity_lower or
                        ' for ' in entity_lower or
                        '&' in entity_text):
                    continue
                categorized_entities["General"].append(normalized)
                flattened.append(entity_text)

            if flattened:
                return flattened, categorized_entities
            else:
                raise APIErrorException("No valid entities extracted from user preference extraction")

        else:
            raise APIErrorException("Invalid result format from user preference extraction")

    except json.JSONDecodeError as e:
        print(f"JSON parsing error in user preference extraction: {e}", flush=True)
        raise APIErrorException("JSON parsing failed in user preference extraction")
    except Exception as e:
        print(f"Unexpected error processing user preference extraction response: {e}", flush=True)
        raise APIErrorException("Response processing failed in user preference extraction")

def prepare_content_and_extract_entities(data_source, data_type: str, llm_model, asin: str = None) -> str:
    """é€šç”¨å‡½æ•°ï¼šå‡†å¤‡å†…å®¹å¹¶æå–å®ä½“ï¼ˆåªè°ƒç”¨LLMï¼Œä¸è§£æJSONï¼‰

    Args:
        data_source: æ•°æ®æºï¼ˆäº§å“ä¿¡æ¯å­—å…¸æˆ–è¯„è®ºåˆ—è¡¨ï¼‰
        data_type: æ•°æ®ç±»å‹ ('product' æˆ– 'user preference')
        llm_model: LLMæ¨¡å‹
        asin: äº§å“ASINï¼ˆå¯é€‰ï¼‰

    Returns:
        åŸå§‹å“åº”å­—ç¬¦ä¸²ï¼ˆå°†è¢«ä¿å­˜åˆ° api_raw_responses.jsonï¼Œç¨åç»Ÿä¸€è§£æï¼‰
    """
    if data_type in ['user_preference', 'user preference']:
        # å¤„ç†ç”¨æˆ·è¯„è®º
        user_reviews = data_source
        if not user_reviews:
            raise APIErrorException("No user reviews available for preference extraction")

        # Combine all review content
        content_parts = []
        for review in user_reviews:
            text = review.get('reviewText', '').strip()
            title = review.get('summary', '').strip()

            if text or title:
                review_parts = []
                if title:
                    review_parts.append(f"Title: {title}")
                if text:
                    review_parts.append(f"Review: {text}")
                content_parts.append(' '.join(review_parts))

        content = ' '.join(content_parts)
        # Clean content
        content = clean_html_content(content)

        return extract_user_preference_entities(content, llm_model, asin=asin)
    else:
        raise ValueError(f"Unsupported data type: {data_type}")

def extract_review_from_prompt(prompt: str) -> Tuple[Optional[str], Optional[str]]:
    """
    ä»promptä¸­æå–æ ‡é¢˜å’Œè¯„è®ºæ–‡æœ¬
    
    Args:
        prompt: APIè¯·æ±‚çš„promptå­—ç¬¦ä¸²
    
    Returns:
        (title, review_text) å…ƒç»„
    """
    if not prompt:
        return None, None
    
    # æŸ¥æ‰¾ "Text: Title: ... Review: ..." æ ¼å¼
    text_match = re.search(r'Text:\s*Title:\s*(.+?)\s*Review:\s*(.+?)(?:\n|$)', prompt, re.DOTALL)
    if text_match:
        title = text_match.group(1).strip()
        review = text_match.group(2).strip()
        return title, review
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–æ ¼å¼
    title_match = re.search(r'Title:\s*(.+?)(?:\n|Review:)', prompt, re.DOTALL)
    review_match = re.search(r'Review:\s*(.+?)(?:\n|$)', prompt, re.DOTALL)
    
    title = title_match.group(1).strip() if title_match else None
    review = review_match.group(1).strip() if review_match else None
    
    return title, review

def match_review_to_product(title: str, review_text: str, user_preference_data: Dict) -> Optional[str]:
    """
    é€šè¿‡åŒ¹é…è¯„è®ºæ–‡æœ¬æ‰¾åˆ°å¯¹åº”çš„ASIN
    
    Args:
        title: è¯„è®ºæ ‡é¢˜
        review_text: è¯„è®ºæ–‡æœ¬
        user_preference_data: ç”¨æˆ·åå¥½æ•°æ®å­—å…¸ï¼ŒåŒ…å«productsåˆ—è¡¨
    
    Returns:
        åŒ¹é…çš„ASINï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
    """
    if not title and not review_text:
        return None
    
    # åˆ›å»ºæœç´¢å…³é”®è¯
    title_lower = title.lower().strip() if title else ""
    review_lower = review_text.lower().strip() if review_text else ""
    
    # æå–å…³é”®è¯ï¼ˆå‰50ä¸ªå­—ç¬¦çš„æ ‡é¢˜å’Œå‰100ä¸ªå­—ç¬¦çš„è¯„è®ºï¼‰
    title_key = title_lower[:50] if title_lower else ""
    review_key = review_lower[:100] if review_lower else ""
    
    # åœ¨user_preference_dataä¸­æŸ¥æ‰¾åŒ¹é…çš„è¯„è®º
    best_match = None
    best_score = 0
    
    for product in user_preference_data.get('products', []):
        for review in product.get('review_content', []):
            review_title = review.get('summary', '').strip().lower()
            review_text_content = review.get('reviewText', '').strip().lower()
            
            score = 0
            
            # åŒ¹é…æ ‡é¢˜
            if title_key and review_title:
                # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ¹é…ï¼ˆè‡³å°‘åŒ¹é…å‰30ä¸ªå­—ç¬¦ï¼‰
                if title_key[:30] in review_title or review_title[:30] in title_key:
                    score += 2
            
            # åŒ¹é…è¯„è®ºæ–‡æœ¬
            if review_key and review_text_content:
                # æ£€æŸ¥è¯„è®ºæ˜¯å¦åŒ¹é…ï¼ˆè‡³å°‘åŒ¹é…å‰80ä¸ªå­—ç¬¦ï¼‰
                if review_key[:80] in review_text_content or review_text_content[:80] in review_key:
                    score += 3
            
            # å¦‚æœæ ‡é¢˜å’Œè¯„è®ºéƒ½åŒ¹é…ï¼Œåˆ†æ•°æ›´é«˜
            if score > best_score:
                best_score = score
                best_match = product.get('asin')
    
    # å¦‚æœåˆ†æ•°è¶³å¤Ÿé«˜ï¼Œè¿”å›åŒ¹é…çš„ASIN
    if best_score >= 2:
        return best_match
    
    return None

def parse_responses_from_file(api_responses_file: str, context: str = "user_preference_extraction", 
                              user_preference_data: Optional[Dict] = None) -> Dict[str, Dict]:
    """
    ä» api_raw_responses.json æ–‡ä»¶ä¸­è§£æå“åº”
    
    Args:
        api_responses_file: APIå“åº”æ–‡ä»¶è·¯å¾„
        context: è¦è§£æçš„ä¸Šä¸‹æ–‡ï¼ˆé»˜è®¤ä¸º "user_preference_extraction"ï¼‰
        user_preference_data: å¯é€‰çš„ç”¨æˆ·åå¥½æ•°æ®ï¼Œç”¨äºåŒ¹é…ASIN
    
    Returns:
        å­—å…¸ï¼Œé”®ä¸ºpromptçš„hashæˆ–ç´¢å¼•ï¼ˆæˆ–ASINå¦‚æœæä¾›äº†user_preference_dataï¼‰ï¼Œå€¼ä¸ºè§£æåçš„å®ä½“å­—å…¸
    """
    if not os.path.exists(api_responses_file):
        log_with_timestamp(f"âš ï¸ API responses file not found: {api_responses_file}")
        return {}
    
    try:
        with open(api_responses_file, 'r', encoding='utf-8') as f:
            all_responses = json.load(f)
    except Exception as e:
        log_with_timestamp(f"âŒ Error reading API responses file: {e}")
        return {}
    
    # Filter responses by context
    filtered_responses = [r for r in all_responses if r.get('context') == context and r.get('success', False)]
    
    log_with_timestamp(f"ğŸ“‹ Found {len(filtered_responses)} responses with context '{context}'")
    
    parsed_results = {}
    for idx, response_data in enumerate(filtered_responses):
        try:
            raw_response = response_data.get('raw_response', {})
            content = raw_response.get('content', '')
            
            if not content:
                log_with_timestamp(f"âš ï¸ Empty content in response {idx}")
                continue
            
            # Parse the response
            entities_result = process_user_preference_extraction_response(content)
            
            # Handle tuple return format (list, dict)
            if isinstance(entities_result, tuple):
                entities_list, entities_dict = entities_result
                # Use the dict if available (new format), otherwise use the list
                parsed_entities = entities_dict if entities_dict else entities_list
            else:
                parsed_entities = entities_result
            
            # Prefer exact ASIN from saved meta (required for ASIN mapping)
            key = None
            meta = response_data.get('meta') or {}
            meta_asin = meta.get('asin')
            if isinstance(meta_asin, str) and meta_asin.strip():
                key = meta_asin.strip().upper()
            
            # Fallback to index-based key (no ASIN mapping)
            if not key:
                key = f"response_{idx}"
            
            parsed_results[key] = parsed_entities
            
        except Exception as e:
            log_with_timestamp(f"âš ï¸ Error parsing response {idx}: {e}")
            continue
    
    log_with_timestamp(f"âœ… Successfully parsed {len(parsed_results)} responses")
    return parsed_results

def parse_api_responses_to_user_preferences(api_responses_file: str, user_preference_file: str, 
                                            output_file: Optional[str] = None) -> Dict:
    """
    è§£æAPIå“åº”æ–‡ä»¶å¹¶ç”Ÿæˆå®Œæ•´çš„ç”¨æˆ·åå¥½å®ä½“æ•°æ®
    
    Args:
        api_responses_file: APIå“åº”æ–‡ä»¶è·¯å¾„
        user_preference_file: ç”¨æˆ·åå¥½æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºè·å–ç”¨æˆ·IDå’Œè¯„è®ºå†…å®¹ï¼‰
        output_file: å¯é€‰çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„
    
    Returns:
        ç”¨æˆ·åå¥½å®ä½“æ•°æ®å­—å…¸ï¼Œæ ¼å¼ä¸user_preference_entities.jsonç›¸åŒ
    """
    # è¯»å–ç”¨æˆ·åå¥½æ–‡ä»¶ä»¥è·å–ç”¨æˆ·IDå’Œäº§å“ä¿¡æ¯
    if not os.path.exists(user_preference_file):
        log_with_timestamp(f"âš ï¸ User preference file not found: {user_preference_file}")
        return {}
    
    try:
        with open(user_preference_file, 'r', encoding='utf-8') as f:
            user_preference_data = json.load(f)
    except Exception as e:
        log_with_timestamp(f"âŒ Error reading user preference file: {e}")
        return {}
    
    user_id = user_preference_data.get('user_id', '')
    
    # è¯»å–APIå“åº”æ–‡ä»¶
    if not os.path.exists(api_responses_file):
        log_with_timestamp(f"âš ï¸ API responses file not found: {api_responses_file}")
        return {}
    
    try:
        with open(api_responses_file, 'r', encoding='utf-8') as f:
            all_responses = json.load(f)
    except Exception as e:
        log_with_timestamp(f"âŒ Error reading API responses file: {e}")
        return {}
    
    # è¿‡æ»¤æˆåŠŸçš„å“åº”
    filtered_responses = [
        r for r in all_responses 
        if r.get('context') == 'user_preference_extraction' 
        and r.get('success', False)
    ]
    
    log_with_timestamp(f"âœ… Found {len(filtered_responses)} successful responses")
    
    # æŒ‰ASINç»„ç»‡å“åº”
    asin_responses = defaultdict(list)
    
    for idx, response_data in enumerate(filtered_responses):
        try:
            # ä»…ä½¿ç”¨ä¿å­˜çš„ meta.asinï¼ˆç²¾ç¡®åŒ¹é…ï¼Œä¸ä¾èµ– prompt å­ä¸²ï¼‰
            asin = None
            meta = response_data.get('meta') or {}
            meta_asin = meta.get('asin')
            if isinstance(meta_asin, str) and meta_asin.strip():
                asin = meta_asin.strip().upper()
            
            if not asin:
                log_with_timestamp(f"âš ï¸ Cannot find ASIN for response {idx} (missing meta.asin)")
                continue
            
            # è§£æå“åº”å†…å®¹
            raw_response = response_data.get('raw_response', {})
            content = raw_response.get('content', '')
            
            if not content:
                log_with_timestamp(f"âš ï¸ Empty content in response {idx}")
                continue
            
            # è§£æå®ä½“
            try:
                entities_result = process_user_preference_extraction_response(content)
                
                # Handle tuple return format (list, dict)
                if isinstance(entities_result, tuple):
                    entities_list, entities_dict = entities_result
                    entities = entities_dict if entities_dict else entities_list
                else:
                    entities = entities_result
                
                if entities:
                    asin_responses[asin].append({
                        'entities': entities,
                        'title': title,
                        'review_text': review_text
                    })
            except Exception as e:
                log_with_timestamp(f"âš ï¸ Error parsing entities for response {idx}: {e}")
                continue
                
        except Exception as e:
            log_with_timestamp(f"âš ï¸ Error processing response {idx}: {e}")
            continue
    
    log_with_timestamp(f"âœ… Successfully parsed responses for {len(asin_responses)} products")
    
    # æ„å»ºè¾“å‡ºæ•°æ®
    output_data = {
        'user_id': user_id,
        'products': []
    }
    
    # ä¸ºæ¯ä¸ªASINåˆå¹¶å®ä½“
    for asin, responses in asin_responses.items():
        # åˆå¹¶æ‰€æœ‰å“åº”ä¸­çš„å®ä½“
        merged_entities = {}
        merged_seen = set()  # (category, entity, sentiment)
        
        for response_info in responses:
            entities = response_info['entities']
            if isinstance(entities, dict):
                for category, entity_list in entities.items():
                    category_norm = normalize_category_label(category)
                    if isinstance(entity_list, list):
                        if category_norm not in merged_entities:
                            merged_entities[category_norm] = []
                        # æ·»åŠ æ–°å®ä½“ï¼ˆå»é‡ï¼‰
                        for entity in entity_list:
                            # Normalize to {"entity": ..., "sentiment": ...}
                            if isinstance(entity, str):
                                entity_text = entity.strip()
                                if not entity_text:
                                    continue
                                normalized = {"entity": entity_text, "sentiment": "neutral"}
                            elif isinstance(entity, dict):
                                entity_text = str(entity.get("entity") or entity.get("text") or entity.get("name") or "").strip()
                                if not entity_text:
                                    continue
                                sentiment = str(entity.get("sentiment") or entity.get("polarity") or "").strip().lower()
                                if sentiment not in {"positive", "negative", "neutral"}:
                                    sentiment = "neutral"
                                normalized = {"entity": entity_text, "sentiment": sentiment}
                            else:
                                continue

                            dedupe_key = (category_norm, normalized["entity"], normalized["sentiment"])
                            if dedupe_key in merged_seen:
                                continue
                            merged_seen.add(dedupe_key)
                            merged_entities[category_norm].append(normalized)
        
        # æŸ¥æ‰¾å¯¹åº”çš„äº§å“è¯„è®º
        product_reviews = []
        for product in user_preference_data.get('products', []):
            if product.get('asin') == asin:
                product_reviews = product.get('review_content', [])
                break
        
        # æ·»åŠ åˆ°è¾“å‡º
        output_data['products'].append({
            'asin': asin,
            'user_preference_entities': merged_entities,
            'review_content': product_reviews
        })
    
    # ä¿å­˜è¾“å‡ºæ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if output_file:
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
            log_with_timestamp(f"ğŸ’¾ Saved results to {output_file}")
        except Exception as e:
            log_with_timestamp(f"âŒ Error saving output file: {e}")
    
    return output_data

def extract_user_preference_entities(content: str, llm_model, asin: Optional[str] = None) -> str:
    """Extract user preference entities using LLM - only call LLM and return raw response.
    
    Returns:
        Raw response string from LLM (will be parsed later from api_raw_responses.json)
    """
    # Ensure content is not None or empty
    if not content or not isinstance(content, str):
        raise APIErrorException("Invalid content for user preference extraction")

    prompt = f"""Extract all entities mentioned in the following product review text and categorize them.
Include any products, activities, techniques, materials, tools, brands, or other relevant entities that the user mentions, regardless of whether they like them or not.

**å®ä½“åˆ†ç±»è¦æ±‚:**
å¯¹äºæ¯ä¸ªæå–çš„å®ä½“ï¼Œå¿…é¡»å°†å…¶å½’ç±»ä¸ºä»¥ä¸‹ç±»åˆ«ä¹‹ä¸€ï¼š
[Brand, Material, Dimensions, Quantity, Color, Design, Usage, Selling Point, Safety/Certification, Accessories, Activity, Technique]

**æƒ…æ„Ÿæ ‡ç­¾è¦æ±‚:**
å¯¹æ¯ä¸ªæå–å‡ºæ¥çš„å®ä½“ï¼Œå¢åŠ ä¸€ä¸ªå­—æ®µ `sentiment`ï¼Œè¡¨ç¤ºç”¨æˆ·å¯¹è¯¥å®ä½“çš„æ€åº¦ï¼š
- positive: ç§¯æ/å–œæ¬¢/è®¤å¯/æ¨è/å¤¸èµ
- negative: æ¶ˆæ/ä¸å–œæ¬¢/æŠ±æ€¨/æ‰¹è¯„/ä¸æ¨è
- neutral: æ–‡æœ¬ä¸­æœªä½“ç°æ˜ç¡®æ€åº¦æˆ–æ— æ³•åˆ¤æ–­

**è¾“å‡ºæ ¼å¼:**
è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œå…¶ä¸­é”®æ˜¯ç±»åˆ«åç§°ï¼Œå€¼æ˜¯è¯¥ç±»åˆ«å¯¹åº”çš„å®ä½“æ•°ç»„ã€‚æ•°ç»„å†…æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼š
{{"entity": "<å®ä½“æ–‡æœ¬>", "sentiment": "positive|negative|neutral"}}

ç¤ºä¾‹:
{{
  "Brand": [{{"entity": "Apple", "sentiment": "positive"}}, {{"entity": "Samsung", "sentiment": "negative"}}],
  "Design": [{{"entity": "smartphone", "sentiment": "neutral"}}, {{"entity": "waterproof", "sentiment": "positive"}}],
  "Selling Point": [{{"entity": "battery life", "sentiment": "positive"}}, {{"entity": "camera quality", "sentiment": "neutral"}}],
  "Usage": [{{"entity": "wireless charging", "sentiment": "positive"}}, {{"entity": "fast charging", "sentiment": "positive"}}],
  "Color": [{{"entity": "black", "sentiment": "neutral"}}, {{"entity": "blue", "sentiment": "neutral"}}]
}}

åªè¿”å›æœ‰æ•ˆçš„JSONå¯¹è±¡ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚

Text: {content}"""

    # Just call LLM and return raw response - no JSON parsing here
    # All responses are saved to api_raw_responses.json, will be parsed later
    response_str, success = call_llm_with_retry(
        llm_model,
        prompt,
        context="user_preference_extraction",
        meta={"asin": asin.strip().upper()} if isinstance(asin, str) and asin.strip() else None,
    )
    if success and response_str:
        return response_str
    else:
        raise APIErrorException("Failed to get response from LLM for user preference extraction")